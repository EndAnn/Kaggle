{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd49055",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:02:13.783251Z",
     "iopub.status.busy": "2023-10-30T22:02:13.782898Z",
     "iopub.status.idle": "2023-10-30T22:02:13.792888Z",
     "shell.execute_reply": "2023-10-30T22:02:13.792038Z"
    },
    "papermill": {
     "duration": 0.030987,
     "end_time": "2023-10-30T22:02:13.794906",
     "exception": false,
     "start_time": "2023-10-30T22:02:13.763919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3945c728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:02:13.829790Z",
     "iopub.status.busy": "2023-10-30T22:02:13.829541Z",
     "iopub.status.idle": "2023-10-30T22:03:49.669127Z",
     "shell.execute_reply": "2023-10-30T22:03:49.668134Z"
    },
    "papermill": {
     "duration": 95.859743,
     "end_time": "2023-10-30T22:03:49.671588",
     "exception": false,
     "start_time": "2023-10-30T22:02:13.811845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/autocorrect/autocorrect-2.6.1.tar\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: autocorrect\r\n",
      "  Building wheel for autocorrect (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622383 sha256=13b56d1a4d6137b4c7c61583771ccce583e49d4a9893caa67e3e3dce2b6fa81e\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/db/69/42/0fb0421d2fe70d195a04665edc760cfe5fd341d7bb8d8e0aaa\r\n",
      "Successfully built autocorrect\r\n",
      "Installing collected packages: autocorrect\r\n",
      "Successfully installed autocorrect-2.6.1\r\n",
      "Processing /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\r\n",
      "Installing collected packages: pyspellchecker\r\n",
      "Successfully installed pyspellchecker-0.7.2\r\n",
      "Processing /kaggle/input/pyphen-0100/Pyphen-0.10.0-py3-none-any.whl\r\n",
      "Installing collected packages: Pyphen\r\n",
      "Successfully installed Pyphen-0.10.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"/kaggle/input/autocorrect/autocorrect-2.6.1.tar\"\n",
    "!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"\n",
    "!pip install \"/kaggle/input/pyphen-0100/Pyphen-0.10.0-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b7b45",
   "metadata": {
    "papermill": {
     "duration": 0.017676,
     "end_time": "2023-10-30T22:03:49.708805",
     "exception": false,
     "start_time": "2023-10-30T22:03:49.691129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A dataset of extra data (meta-information) was established by Oleksiy Kononenko.\n",
    "\n",
    "The linkof dataset is https://www.kaggle.com/datasets/kononenko/commonlit-texts/data\n",
    "\n",
    "It has free, standards-aligned articles and passages from renowned authors as scraped from the CommonLit library. \n",
    "\n",
    "I use it to supplement initial dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea432acd",
   "metadata": {
    "papermill": {
     "duration": 0.017851,
     "end_time": "2023-10-30T22:03:49.744422",
     "exception": false,
     "start_time": "2023-10-30T22:03:49.726571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Meta Data creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2136de7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:03:49.782338Z",
     "iopub.status.busy": "2023-10-30T22:03:49.781606Z",
     "iopub.status.idle": "2023-10-30T22:04:18.606682Z",
     "shell.execute_reply": "2023-10-30T22:04:18.605716Z"
    },
    "papermill": {
     "duration": 28.846666,
     "end_time": "2023-10-30T22:04:18.609083",
     "exception": false,
     "start_time": "2023-10-30T22:03:49.762417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load Spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class FeatureEngineering:\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.df['grade'].fillna(0, inplace=True)  # Fill NA values in 'grade' with 0\n",
    "\n",
    "    def classify_author(self, author):\n",
    "        doc = nlp(author)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                return 'person'\n",
    "        return 'org'\n",
    "\n",
    "    def encode_author_type(self):\n",
    "        self.df['author_type'] = self.df['author'].apply(self.classify_author)\n",
    "        le = LabelEncoder()\n",
    "        self.df['author_type'] = le.fit_transform(self.df['author_type'])\n",
    "\n",
    "    def frequency_encoding(self):\n",
    "        logging.info(\"Applying Frequency Encoding on 'author'\")\n",
    "        self.df['author_frequency'] = self.df['author'].map(self.df['author'].value_counts())\n",
    "\n",
    "    def one_hot_encoding(self):\n",
    "        logging.info(\"Applying One-Hot Encoding on 'genre'\")\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        genre_onehot = onehot_encoder.fit_transform(self.df[['genre']])\n",
    "        df_onehot = pd.DataFrame(genre_onehot, columns=onehot_encoder.get_feature_names_out(['genre']))\n",
    "        self.df = pd.concat([self.df, df_onehot], axis=1)\n",
    "\n",
    "    def feature_scaling(self):\n",
    "        logging.info(\"Applying Feature Scaling on 'lexile'\")\n",
    "        scaler = StandardScaler()\n",
    "        self.df['lexile_scaled'] = scaler.fit_transform(self.df[['lexile']])\n",
    "\n",
    "    def transform(self):\n",
    "        self.encode_author_type()\n",
    "        self.frequency_encoding()\n",
    "        self.feature_scaling()\n",
    "        return self.df\n",
    "\n",
    "# Initialize FeatureEngineering class and apply transformations\n",
    "prompt_grade = pd.read_csv(r'/kaggle/input/commonlit-texts/commonlit_texts.csv')\n",
    "feature_engineer = FeatureEngineering(prompt_grade)\n",
    "transformed_df = feature_engineer.transform()\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "prompt_grade = transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12ccc5da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:04:18.647925Z",
     "iopub.status.busy": "2023-10-30T22:04:18.647617Z",
     "iopub.status.idle": "2023-10-30T22:04:18.676211Z",
     "shell.execute_reply": "2023-10-30T22:04:18.675354Z"
    },
    "papermill": {
     "duration": 0.05034,
     "end_time": "2023-10-30T22:04:18.678271",
     "exception": false,
     "start_time": "2023-10-30T22:04:18.627931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>description</th>\n",
       "      <th>grade</th>\n",
       "      <th>genre</th>\n",
       "      <th>lexile</th>\n",
       "      <th>path</th>\n",
       "      <th>is_prose</th>\n",
       "      <th>date</th>\n",
       "      <th>intro</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>license</th>\n",
       "      <th>notes</th>\n",
       "      <th>author_type</th>\n",
       "      <th>author_frequency</th>\n",
       "      <th>lexile_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(love song, with two goldfish)</td>\n",
       "      <td>Grace Chua</td>\n",
       "      <td>The speaker describes a love story between two...</td>\n",
       "      <td>8</td>\n",
       "      <td>Poem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/en/texts/love-song-with-two-goldfish</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>Grace Chua is an award-winning journalist whos...</td>\n",
       "      <td>[1]\\n(He's a drifter,\\nA “drifter” is a person...</td>\n",
       "      <td>\"(love song, with two goldfish)\" from\\n Quarte...</td>\n",
       "      <td>A “drifter” is a person who is continually mov...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 Things You Can Do to Avoid Fraud</td>\n",
       "      <td>Federal Trade Commission</td>\n",
       "      <td>The Federal Trade Commission discusses what pe...</td>\n",
       "      <td>10</td>\n",
       "      <td>Informational Text</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>/en/texts/10-things-you-can-do-to-avoid-fraud</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Whether they come in the form of an email clai...</td>\n",
       "      <td>[1]\\nInternational scam artists use clever sch...</td>\n",
       "      <td>\"10 Things You Can Do to Avoid Fraud\" by Feder...</td>\n",
       "      <td>to illegally obtain money from someone\\nFoil\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.463568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100 years ago: An election, a virus and a cry ...</td>\n",
       "      <td>Michael E. Ruane</td>\n",
       "      <td>In 2020, a news reporter takes a look back at ...</td>\n",
       "      <td>11</td>\n",
       "      <td>Informational Text</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>/en/texts/100-years-ago-an-election-a-virus-an...</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>Michael E. Ruane is a general assignment repor...</td>\n",
       "      <td>[1]\\nA critical election loomed. The country w...</td>\n",
       "      <td>\"100 years ago: An election, a virus and a cry...</td>\n",
       "      <td>significant decline in economic activity\\nStri...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.625670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Concussions</td>\n",
       "      <td>Casey Cochran</td>\n",
       "      <td>In this article, a former college football pla...</td>\n",
       "      <td>9</td>\n",
       "      <td>Informational Text</td>\n",
       "      <td>810.0</td>\n",
       "      <td>/en/texts/13-concussions</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>Casey Cochran played college football at the U...</td>\n",
       "      <td>[1]\\nIt was a beautiful night in late August. ...</td>\n",
       "      <td>\"13 Concussions\" from \\nThe Players' Tribune\\n...</td>\n",
       "      <td>Brigham Young University - the Cougar is their...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.711674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 13th</td>\n",
       "      <td>Shelley Walden</td>\n",
       "      <td>When the COVID-19 lockdown begins, a superstit...</td>\n",
       "      <td>6</td>\n",
       "      <td>Short Story</td>\n",
       "      <td>700.0</td>\n",
       "      <td>/en/texts/the-13th</td>\n",
       "      <td>1</td>\n",
       "      <td>2021</td>\n",
       "      <td>Superstitions exist in many different cultures...</td>\n",
       "      <td>[1]\\nAs Angela stared out the school bus windo...</td>\n",
       "      <td>\"The 13th\" by Shelley Walden. Copyright © 202...</td>\n",
       "      <td>Coincidence\\n \\n(noun) : \\nan event that happe...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.157455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2394</th>\n",
       "      <td>Yul Kwon, From Bullying Target to Reality TV Star</td>\n",
       "      <td>NPR Staff</td>\n",
       "      <td>Yul Kwon reflects on his decision to join the ...</td>\n",
       "      <td>9</td>\n",
       "      <td>News</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>/en/texts/yul-kwon-from-bullying-target-to-rea...</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>Yul Kwon's early life was mired with a host of...</td>\n",
       "      <td>[1]\\nYul Kwon first earned his game-changer st...</td>\n",
       "      <td>©2012 National Public Radio, Inc. News report ...</td>\n",
       "      <td>Phi Beta Kappa is an honorary society of colle...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.868824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>Yusuf and the Great Big Brownie Mistake</td>\n",
       "      <td>Aisha Saeed</td>\n",
       "      <td>A boy burns the brownies he bakes for his fami...</td>\n",
       "      <td>5</td>\n",
       "      <td>Short Story</td>\n",
       "      <td>680.0</td>\n",
       "      <td>/en/texts/yusuf-and-the-great-big-brownie-mistake</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>Aisha Saeed is the New York Times-bestselling ...</td>\n",
       "      <td>[1]\\nEid lights twinkled along the curved entr...</td>\n",
       "      <td>From ONCE UPON AN EID edited by S. K. Ali and ...</td>\n",
       "      <td>Traditional\\n \\n(adjective) : \\nof the ways of...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.238506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>Zap It!</td>\n",
       "      <td>Tracy Vonder Brink</td>\n",
       "      <td>Tracy Vonder Brink explains the science behind...</td>\n",
       "      <td>4</td>\n",
       "      <td>Informational Text</td>\n",
       "      <td>640.0</td>\n",
       "      <td>/en/texts/zap-it</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>In this informational text, Tracy Vonder Brink...</td>\n",
       "      <td>[1]\\nIn 1946, a scientist named Percy Spencer ...</td>\n",
       "      <td>\"Zap It!\" by Tracy Vonder Brink. Copyright © 2...</td>\n",
       "      <td>Energy\\n \\n(noun) : \\nthe power needed to do w...</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>-1.400609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>Zebra and Wasp</td>\n",
       "      <td>Clare Mishica</td>\n",
       "      <td>A zebra helps a wasp trapped in a spider web.</td>\n",
       "      <td>3</td>\n",
       "      <td>Fable</td>\n",
       "      <td>540.0</td>\n",
       "      <td>/en/texts/zebra-and-wasp</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>Clare Mishica has written for \\nHighlights\\n. ...</td>\n",
       "      <td>[1]\\nOne sunny morning, Zebra visited the rive...</td>\n",
       "      <td>All Highlights material is copyrighted by High...</td>\n",
       "      <td>Wail\\n \\n(verb) : \\nto cry out in pain\\na larg...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.805864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>Zombies are real!</td>\n",
       "      <td>Kathryn Hulick</td>\n",
       "      <td>Kathryn Hulick discusses real-life zombies and...</td>\n",
       "      <td>8</td>\n",
       "      <td>Informational Text</td>\n",
       "      <td>960.0</td>\n",
       "      <td>/en/texts/zombies-are-real</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>Zombies are terrifying, but they're the work o...</td>\n",
       "      <td>[1]\\nA zombie crawls through the forest. When ...</td>\n",
       "      <td>From \\nScience News for Students\\n,  October 2...</td>\n",
       "      <td>a stem or support\\nthe destruction of the worl...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.103790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2399 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                        (love song, with two goldfish)   \n",
       "1                   10 Things You Can Do to Avoid Fraud   \n",
       "2     100 years ago: An election, a virus and a cry ...   \n",
       "3                                        13 Concussions   \n",
       "4                                              The 13th   \n",
       "...                                                 ...   \n",
       "2394  Yul Kwon, From Bullying Target to Reality TV Star   \n",
       "2395            Yusuf and the Great Big Brownie Mistake   \n",
       "2396                                            Zap It!   \n",
       "2397                                     Zebra and Wasp   \n",
       "2398                                  Zombies are real!   \n",
       "\n",
       "                        author  \\\n",
       "0                   Grace Chua   \n",
       "1     Federal Trade Commission   \n",
       "2             Michael E. Ruane   \n",
       "3                Casey Cochran   \n",
       "4               Shelley Walden   \n",
       "...                        ...   \n",
       "2394                 NPR Staff   \n",
       "2395               Aisha Saeed   \n",
       "2396        Tracy Vonder Brink   \n",
       "2397             Clare Mishica   \n",
       "2398            Kathryn Hulick   \n",
       "\n",
       "                                            description  grade  \\\n",
       "0     The speaker describes a love story between two...      8   \n",
       "1     The Federal Trade Commission discusses what pe...     10   \n",
       "2     In 2020, a news reporter takes a look back at ...     11   \n",
       "3     In this article, a former college football pla...      9   \n",
       "4     When the COVID-19 lockdown begins, a superstit...      6   \n",
       "...                                                 ...    ...   \n",
       "2394  Yul Kwon reflects on his decision to join the ...      9   \n",
       "2395  A boy burns the brownies he bakes for his fami...      5   \n",
       "2396  Tracy Vonder Brink explains the science behind...      4   \n",
       "2397      A zebra helps a wasp trapped in a spider web.      3   \n",
       "2398  Kathryn Hulick discusses real-life zombies and...      8   \n",
       "\n",
       "                   genre  lexile  \\\n",
       "0                   Poem     NaN   \n",
       "1     Informational Text  1100.0   \n",
       "2     Informational Text  1140.0   \n",
       "3     Informational Text   810.0   \n",
       "4            Short Story   700.0   \n",
       "...                  ...     ...   \n",
       "2394                News  1200.0   \n",
       "2395         Short Story   680.0   \n",
       "2396  Informational Text   640.0   \n",
       "2397               Fable   540.0   \n",
       "2398  Informational Text   960.0   \n",
       "\n",
       "                                                   path  is_prose  date  \\\n",
       "0                 /en/texts/love-song-with-two-goldfish         0  2003   \n",
       "1         /en/texts/10-things-you-can-do-to-avoid-fraud         1   NaN   \n",
       "2     /en/texts/100-years-ago-an-election-a-virus-an...         1  2020   \n",
       "3                              /en/texts/13-concussions         1  2016   \n",
       "4                                    /en/texts/the-13th         1  2021   \n",
       "...                                                 ...       ...   ...   \n",
       "2394  /en/texts/yul-kwon-from-bullying-target-to-rea...         1  2012   \n",
       "2395  /en/texts/yusuf-and-the-great-big-brownie-mistake         1  2020   \n",
       "2396                                   /en/texts/zap-it         1  2022   \n",
       "2397                           /en/texts/zebra-and-wasp         1  2017   \n",
       "2398                         /en/texts/zombies-are-real         1  2016   \n",
       "\n",
       "                                                  intro  \\\n",
       "0     Grace Chua is an award-winning journalist whos...   \n",
       "1     Whether they come in the form of an email clai...   \n",
       "2     Michael E. Ruane is a general assignment repor...   \n",
       "3     Casey Cochran played college football at the U...   \n",
       "4     Superstitions exist in many different cultures...   \n",
       "...                                                 ...   \n",
       "2394  Yul Kwon's early life was mired with a host of...   \n",
       "2395  Aisha Saeed is the New York Times-bestselling ...   \n",
       "2396  In this informational text, Tracy Vonder Brink...   \n",
       "2397  Clare Mishica has written for \\nHighlights\\n. ...   \n",
       "2398  Zombies are terrifying, but they're the work o...   \n",
       "\n",
       "                                                excerpt  \\\n",
       "0     [1]\\n(He's a drifter,\\nA “drifter” is a person...   \n",
       "1     [1]\\nInternational scam artists use clever sch...   \n",
       "2     [1]\\nA critical election loomed. The country w...   \n",
       "3     [1]\\nIt was a beautiful night in late August. ...   \n",
       "4     [1]\\nAs Angela stared out the school bus windo...   \n",
       "...                                                 ...   \n",
       "2394  [1]\\nYul Kwon first earned his game-changer st...   \n",
       "2395  [1]\\nEid lights twinkled along the curved entr...   \n",
       "2396  [1]\\nIn 1946, a scientist named Percy Spencer ...   \n",
       "2397  [1]\\nOne sunny morning, Zebra visited the rive...   \n",
       "2398  [1]\\nA zombie crawls through the forest. When ...   \n",
       "\n",
       "                                                license  \\\n",
       "0     \"(love song, with two goldfish)\" from\\n Quarte...   \n",
       "1     \"10 Things You Can Do to Avoid Fraud\" by Feder...   \n",
       "2     \"100 years ago: An election, a virus and a cry...   \n",
       "3     \"13 Concussions\" from \\nThe Players' Tribune\\n...   \n",
       "4      \"The 13th\" by Shelley Walden. Copyright © 202...   \n",
       "...                                                 ...   \n",
       "2394  ©2012 National Public Radio, Inc. News report ...   \n",
       "2395  From ONCE UPON AN EID edited by S. K. Ali and ...   \n",
       "2396  \"Zap It!\" by Tracy Vonder Brink. Copyright © 2...   \n",
       "2397  All Highlights material is copyrighted by High...   \n",
       "2398  From \\nScience News for Students\\n,  October 2...   \n",
       "\n",
       "                                                  notes  author_type  \\\n",
       "0     A “drifter” is a person who is continually mov...            1   \n",
       "1     to illegally obtain money from someone\\nFoil\\n...            0   \n",
       "2     significant decline in economic activity\\nStri...            1   \n",
       "3     Brigham Young University - the Cougar is their...            1   \n",
       "4     Coincidence\\n \\n(noun) : \\nan event that happe...            1   \n",
       "...                                                 ...          ...   \n",
       "2394  Phi Beta Kappa is an honorary society of colle...            0   \n",
       "2395  Traditional\\n \\n(adjective) : \\nof the ways of...            1   \n",
       "2396  Energy\\n \\n(noun) : \\nthe power needed to do w...            1   \n",
       "2397  Wail\\n \\n(verb) : \\nto cry out in pain\\na larg...            0   \n",
       "2398  a stem or support\\nthe destruction of the worl...            1   \n",
       "\n",
       "      author_frequency  lexile_scaled  \n",
       "0                    2            NaN  \n",
       "1                    1       0.463568  \n",
       "2                    1       0.625670  \n",
       "3                    1      -0.711674  \n",
       "4                    2      -1.157455  \n",
       "...                ...            ...  \n",
       "2394                 9       0.868824  \n",
       "2395                 1      -1.238506  \n",
       "2396                25      -1.400609  \n",
       "2397                 4      -1.805864  \n",
       "2398                 2      -0.103790  \n",
       "\n",
       "[2399 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8574834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:04:18.717203Z",
     "iopub.status.busy": "2023-10-30T22:04:18.716908Z",
     "iopub.status.idle": "2023-10-30T22:04:18.722914Z",
     "shell.execute_reply": "2023-10-30T22:04:18.721987Z"
    },
    "papermill": {
     "duration": 0.027344,
     "end_time": "2023-10-30T22:04:18.724730",
     "exception": false,
     "start_time": "2023-10-30T22:04:18.697386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keep_columns = ['title','author','description','grade','genre','lexile','lexile_scaled','is_prose','author_type','author_frequency']\n",
    "prompt_grade = prompt_grade[keep_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3abe0d",
   "metadata": {
    "papermill": {
     "duration": 0.018547,
     "end_time": "2023-10-30T22:04:18.761692",
     "exception": false,
     "start_time": "2023-10-30T22:04:18.743145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c571bd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:04:18.799833Z",
     "iopub.status.busy": "2023-10-30T22:04:18.799527Z",
     "iopub.status.idle": "2023-10-30T22:04:23.846982Z",
     "shell.execute_reply": "2023-10-30T22:04:23.846135Z"
    },
    "papermill": {
     "duration": 5.069215,
     "end_time": "2023-10-30T22:04:23.849317",
     "exception": false,
     "start_time": "2023-10-30T22:04:18.780102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import get_polynomial_decay_schedule_with_warmup,get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorWithPadding,DataCollatorForTokenClassification\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, disable_progress_bar\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob\n",
    "import optuna\n",
    "import optuna.integration.lightgbm as lgb\n",
    "import pyphen\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import Counter\n",
    "from nltk import ne_chunk, word_tokenize, pos_tag\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from text_unidecode import unidecode\n",
    "from typing import Dict, List, Tuple\n",
    "import codecs\n",
    "from datasets import concatenate_datasets,load_dataset,load_from_disk\n",
    "import spacy\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "from spellchecker import SpellChecker\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import collections\n",
    "from termcolor import colored\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "disable_progress_bar()\n",
    "tqdm.pandas()\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b4a101c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:04:23.888732Z",
     "iopub.status.busy": "2023-10-30T22:04:23.888519Z",
     "iopub.status.idle": "2023-10-30T22:04:24.025504Z",
     "shell.execute_reply": "2023-10-30T22:04:24.024534Z"
    },
    "papermill": {
     "duration": 0.159451,
     "end_time": "2023-10-30T22:04:24.027898",
     "exception": false,
     "start_time": "2023-10-30T22:04:23.868447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n",
    "prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\n",
    "prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n",
    "summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\n",
    "summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n",
    "sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aacc0d3",
   "metadata": {
    "papermill": {
     "duration": 0.018804,
     "end_time": "2023-10-30T22:04:24.066058",
     "exception": false,
     "start_time": "2023-10-30T22:04:24.047254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Joining prompt and meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ae759a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:04:24.104803Z",
     "iopub.status.busy": "2023-10-30T22:04:24.104566Z",
     "iopub.status.idle": "2023-10-30T22:04:24.135190Z",
     "shell.execute_reply": "2023-10-30T22:04:24.134251Z"
    },
    "papermill": {
     "duration": 0.05281,
     "end_time": "2023-10-30T22:04:24.137444",
     "exception": false,
     "start_time": "2023-10-30T22:04:24.084634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_and_join(df1, df2, df1_title_col, df2_title_col, grade_col):\n",
    "    # Copy dataframes to avoid modifying the originals\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "\n",
    "    # Preprocess titles\n",
    "    df1[df1_title_col] = df1[df1_title_col].str.replace('\"', '').str.strip()\n",
    "    df2[df2_title_col] = df2[df2_title_col].str.replace('\"', '').str.strip()\n",
    "\n",
    "    df2 = df2.drop_duplicates(subset=df2_title_col, keep='first')\n",
    "\n",
    "    merged_df = df1.merge(df2, how='left', left_on=df1_title_col, right_on=df2_title_col)\n",
    "    \n",
    "    merged_df[grade_col] = merged_df[grade_col].fillna(0)\n",
    "    merged_df[grade_col] = merged_df[grade_col].astype(int).astype('category')\n",
    "    return merged_df\n",
    "\n",
    "prompts_train = preprocess_and_join(prompts_train, prompt_grade, 'prompt_title', 'title', 'grade')\n",
    "prompts_test = preprocess_and_join(prompts_test, prompt_grade, 'prompt_title', 'title', 'grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa97034",
   "metadata": {
    "papermill": {
     "duration": 0.018906,
     "end_time": "2023-10-30T22:04:24.176623",
     "exception": false,
     "start_time": "2023-10-30T22:04:24.157717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdbf5c9",
   "metadata": {
    "papermill": {
     "duration": 0.018601,
     "end_time": "2023-10-30T22:04:24.213783",
     "exception": false,
     "start_time": "2023-10-30T22:04:24.195182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here are descriptions parameters of the trained Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dff5748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:04:24.255307Z",
     "iopub.status.busy": "2023-10-30T22:04:24.255056Z",
     "iopub.status.idle": "2023-10-30T22:04:24.260646Z",
     "shell.execute_reply": "2023-10-30T22:04:24.259894Z"
    },
    "papermill": {
     "duration": 0.028995,
     "end_time": "2023-10-30T22:04:24.262580",
     "exception": false,
     "start_time": "2023-10-30T22:04:24.233585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name=\"/kaggle/input/debertav3base\"\n",
    "    preproc_type=4\n",
    "    pooling=\"ClsPooling\"\n",
    "    learning_rate=1.5e-5\n",
    "    warmup_ratio=0.01\n",
    "    weight_decay=0.02\n",
    "    hidden_dropout_prob=0.005\n",
    "    attention_probs_dropout_prob=0.005\n",
    "    num_layers_to_freeze=99\n",
    "    num_train_epochs=5\n",
    "    n_splits=4\n",
    "    batch_size=12\n",
    "    random_seed=42\n",
    "    save_steps=75\n",
    "    max_length=512\n",
    "    adjustment_factor= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739def04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:04:24.302821Z",
     "iopub.status.busy": "2023-10-30T22:04:24.302603Z",
     "iopub.status.idle": "2023-10-30T22:04:24.321491Z",
     "shell.execute_reply": "2023-10-30T22:04:24.320552Z"
    },
    "papermill": {
     "duration": 0.041589,
     "end_time": "2023-10-30T22:04:24.324009",
     "exception": false,
     "start_time": "2023-10-30T22:04:24.282420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>description</th>\n",
       "      <th>grade</th>\n",
       "      <th>genre</th>\n",
       "      <th>lexile</th>\n",
       "      <th>lexile_scaled</th>\n",
       "      <th>is_prose</th>\n",
       "      <th>author_type</th>\n",
       "      <th>author_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Aristotle</td>\n",
       "      <td>This excerpt from Aristotle's famous work \"Poe...</td>\n",
       "      <td>9</td>\n",
       "      <td>Philosophy</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>0.341991</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3b9047</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>USHistory.org</td>\n",
       "      <td>This informational text describes the social s...</td>\n",
       "      <td>7</td>\n",
       "      <td>Informational Text</td>\n",
       "      <td>890.0</td>\n",
       "      <td>-0.387469</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>814d6b</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>CommonLit Staff</td>\n",
       "      <td>In 1967, a history teacher's social experiment...</td>\n",
       "      <td>9</td>\n",
       "      <td>Informational Text</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>1.111977</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>Upton Sinclair</td>\n",
       "      <td>In this disturbing piece of political fiction,...</td>\n",
       "      <td>11</td>\n",
       "      <td>Fiction - General</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1.679335</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_id                                    prompt_question  \\\n",
       "0    39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "1    3b9047  In complete sentences, summarize the structure...   \n",
       "2    814d6b  Summarize how the Third Wave developed over su...   \n",
       "3    ebad26  Summarize the various ways the factory would u...   \n",
       "\n",
       "                prompt_title  \\\n",
       "0                 On Tragedy   \n",
       "1  Egyptian Social Structure   \n",
       "2             The Third Wave   \n",
       "3    Excerpt from The Jungle   \n",
       "\n",
       "                                         prompt_text  \\\n",
       "0  Chapter 13 \\r\\nAs the sequel to what has alrea...   \n",
       "1  Egyptian society was structured like a pyramid...   \n",
       "2  Background \\r\\nThe Third Wave experiment took ...   \n",
       "3  With one member trimming beef in a cannery, an...   \n",
       "\n",
       "                       title           author  \\\n",
       "0                 On Tragedy        Aristotle   \n",
       "1  Egyptian Social Structure    USHistory.org   \n",
       "2             The Third Wave  CommonLit Staff   \n",
       "3    Excerpt from The Jungle   Upton Sinclair   \n",
       "\n",
       "                                         description grade  \\\n",
       "0  This excerpt from Aristotle's famous work \"Poe...     9   \n",
       "1  This informational text describes the social s...     7   \n",
       "2  In 1967, a history teacher's social experiment...     9   \n",
       "3  In this disturbing piece of political fiction,...    11   \n",
       "\n",
       "                genre  lexile  lexile_scaled  is_prose  author_type  \\\n",
       "0          Philosophy  1070.0       0.341991         1            1   \n",
       "1  Informational Text   890.0      -0.387469         1            0   \n",
       "2  Informational Text  1260.0       1.111977         1            0   \n",
       "3   Fiction - General  1400.0       1.679335         1            0   \n",
       "\n",
       "   author_frequency  \n",
       "0                 2  \n",
       "1                42  \n",
       "2                24  \n",
       "3                 1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de15b82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:04:24.371109Z",
     "iopub.status.busy": "2023-10-30T22:04:24.370879Z",
     "iopub.status.idle": "2023-10-30T22:04:24.387287Z",
     "shell.execute_reply": "2023-10-30T22:04:24.386368Z"
    },
    "papermill": {
     "duration": 0.041509,
     "end_time": "2023-10-30T22:04:24.389529",
     "exception": false,
     "start_time": "2023-10-30T22:04:24.348020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>description</th>\n",
       "      <th>grade</th>\n",
       "      <th>genre</th>\n",
       "      <th>lexile</th>\n",
       "      <th>lexile_scaled</th>\n",
       "      <th>is_prose</th>\n",
       "      <th>author_type</th>\n",
       "      <th>author_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abc123</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def789</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_id prompt_question     prompt_title       prompt_text title author  \\\n",
       "0    abc123    Summarize...  Example Title 1  Heading\\nText...   NaN    NaN   \n",
       "1    def789    Summarize...  Example Title 2  Heading\\nText...   NaN    NaN   \n",
       "\n",
       "  description grade genre  lexile  lexile_scaled  is_prose  author_type  \\\n",
       "0         NaN     0   NaN     NaN            NaN       NaN          NaN   \n",
       "1         NaN     0   NaN     NaN            NaN       NaN          NaN   \n",
       "\n",
       "   author_frequency  \n",
       "0               NaN  \n",
       "1               NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f506e",
   "metadata": {
    "papermill": {
     "duration": 0.021169,
     "end_time": "2023-10-30T22:04:24.431871",
     "exception": false,
     "start_time": "2023-10-30T22:04:24.410702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature engineering based on prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44231b",
   "metadata": {
    "papermill": {
     "duration": 0.020817,
     "end_time": "2023-10-30T22:04:24.473965",
     "exception": false,
     "start_time": "2023-10-30T22:04:24.453148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Feature Engineering is perfomed by code https://www.kaggle.com/code/tsunotsuno/updated-debertav3-lgbm-with-spell-autocorrect\n",
    "\n",
    "* Text Length\n",
    "* Length Ratio\n",
    "* Word Overlap\n",
    "* N-grams Co-occurrence\n",
    "* Count\n",
    "* Ratio\n",
    "* Quotes Overlap\n",
    "* Grammar Check\n",
    "* Spelling: pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34befd03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:04:24.520421Z",
     "iopub.status.busy": "2023-10-30T22:04:24.520135Z",
     "iopub.status.idle": "2023-10-30T22:04:27.210155Z",
     "shell.execute_reply": "2023-10-30T22:04:27.209189Z"
    },
    "papermill": {
     "duration": 2.716409,
     "end_time": "2023-10-30T22:04:27.212575",
     "exception": false,
     "start_time": "2023-10-30T22:04:24.496166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic = pyphen.Pyphen(lang='en')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, \n",
    "                model_name: str,\n",
    "                ) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"{model_name}\")\n",
    "        self.twd = TreebankWordDetokenizer()\n",
    "        self.STOP_WORDS = set(stopwords.words('english'))\n",
    "        \n",
    "        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n",
    "        self.speller = Speller(lang='en')\n",
    "        self.spellchecker = SpellChecker() \n",
    "        \n",
    "    def calculate_text_similarity(self, row):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform([row['prompt_text'], row['text']])\n",
    "        return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2]).flatten()[0]\n",
    "    \n",
    "    def sentiment_analysis(self, text):\n",
    "        analysis = TextBlob(text)\n",
    "        return analysis.sentiment.polarity, analysis.sentiment.subjectivity\n",
    "    \n",
    "    def word_overlap_count(self, row):\n",
    "        \"\"\" intersection(prompt_text, text) \"\"\"        \n",
    "        def check_is_stop_word(word):\n",
    "            return word in self.STOP_WORDS\n",
    "        \n",
    "        prompt_words = row['prompt_tokens']\n",
    "        summary_words = row['summary_tokens']\n",
    "        if self.STOP_WORDS:\n",
    "            prompt_words = list(filter(check_is_stop_word, prompt_words))\n",
    "            summary_words = list(filter(check_is_stop_word, summary_words))\n",
    "        return len(set(prompt_words).intersection(set(summary_words)))\n",
    "            \n",
    "    def ngrams(self, token, n):\n",
    "        ngrams = zip(*[token[i:] for i in range(n)])\n",
    "        return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "    def ngram_co_occurrence(self, row, n: int) -> int:\n",
    "        original_tokens = row['prompt_tokens']\n",
    "        summary_tokens = row['summary_tokens']\n",
    "\n",
    "        # Generate n-grams for the original text and summary\n",
    "        original_ngrams = set(self.ngrams(original_tokens, n))\n",
    "        summary_ngrams = set(self.ngrams(summary_tokens, n))\n",
    "\n",
    "        # Calculate the number of common n-grams\n",
    "        common_ngrams = original_ngrams.intersection(summary_ngrams)\n",
    "        return len(common_ngrams)\n",
    "    \n",
    "    def ner_overlap_count(self, row, mode:str):\n",
    "        model = self.spacy_ner_model\n",
    "        def clean_ners(ner_list):\n",
    "            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n",
    "        prompt = model(row['prompt_text'])\n",
    "        summary = model(row['text'])\n",
    "\n",
    "        if \"spacy\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n",
    "        elif \"stanza\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.type) for token in summary.ents])\n",
    "        else:\n",
    "            raise Exception(\"Model not supported\")\n",
    "\n",
    "        prompt_ner = clean_ners(prompt_ner)\n",
    "        summary_ner = clean_ners(summary_ner)\n",
    "\n",
    "        intersecting_ners = prompt_ner.intersection(summary_ner)\n",
    "        \n",
    "        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            return ner_dict\n",
    "        elif mode == \"test\":\n",
    "            return {key: ner_dict.get(key) for key in self.ner_keys}\n",
    "\n",
    "    \n",
    "    def quotes_count(self, row):\n",
    "        summary = row['text']\n",
    "        text = row['prompt_text']\n",
    "        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n",
    "        if len(quotes_from_summary)>0:\n",
    "            return [quote in text for quote in quotes_from_summary].count(True)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def spelling(self, text):\n",
    "        \n",
    "        wordlist=text.split()\n",
    "        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n",
    "\n",
    "        return amount_miss\n",
    "    \n",
    "    def calculate_unique_words(self,text):\n",
    "        unique_words = set(text.split())\n",
    "        return len(unique_words)\n",
    "    \n",
    "    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n",
    "        self.spellchecker.word_frequency.load_words(tokens)\n",
    "        self.speller.nlp_data.update({token:1000 for token in tokens})\n",
    "        \n",
    "    def calculate_pos_ratios(self , text):\n",
    "        pos_tags = pos_tag(nltk.word_tokenize(text))\n",
    "        pos_counts = Counter(tag for word, tag in pos_tags)\n",
    "        total_words = len(pos_tags)\n",
    "        ratios = {tag: count / total_words for tag, count in pos_counts.items()}\n",
    "        return ratios\n",
    "    \n",
    "    def calculate_punctuation_ratios(self,text):\n",
    "        total_chars = len(text)\n",
    "        punctuation_counts = Counter(char for char in text if char in '.,!?;:\"()[]{}')\n",
    "        ratios = {char: count / total_chars for char, count in punctuation_counts.items()}\n",
    "        return ratios\n",
    "    \n",
    "    def calculate_keyword_density(self,row):\n",
    "        keywords = set(row['prompt_text'].split())\n",
    "        text_words = row['text'].split()\n",
    "        keyword_count = sum(1 for word in text_words if word in keywords)\n",
    "        return keyword_count / len(text_words)\n",
    "    \n",
    "    def count_syllables(self,word):\n",
    "        hyphenated_word = dic.inserted(word)\n",
    "        return len(hyphenated_word.split('-'))\n",
    "\n",
    "    def flesch_reading_ease_manual(self,text):\n",
    "        total_sentences = len(TextBlob(text).sentences)\n",
    "        total_words = len(TextBlob(text).words)\n",
    "        total_syllables = sum(self.count_syllables(word) for word in TextBlob(text).words)\n",
    "\n",
    "        if total_sentences == 0 or total_words == 0:\n",
    "            return 0\n",
    "\n",
    "        flesch_score = 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (total_syllables / total_words)\n",
    "        return flesch_score\n",
    "    \n",
    "    def flesch_kincaid_grade_level(self, text):\n",
    "        total_sentences = len(TextBlob(text).sentences)\n",
    "        total_words = len(TextBlob(text).words)\n",
    "        total_syllables = sum(self.count_syllables(word) for word in TextBlob(text).words)\n",
    "\n",
    "        if total_sentences == 0 or total_words == 0:\n",
    "            return 0\n",
    "\n",
    "        fk_grade = 0.39 * (total_words / total_sentences) + 11.8 * (total_syllables / total_words) - 15.59\n",
    "        return fk_grade\n",
    "    \n",
    "    def gunning_fog(self, text):\n",
    "        total_sentences = len(TextBlob(text).sentences)\n",
    "        total_words = len(TextBlob(text).words)\n",
    "        complex_words = sum(1 for word in TextBlob(text).words if self.count_syllables(word) > 2)\n",
    "\n",
    "        if total_sentences == 0 or total_words == 0:\n",
    "            return 0\n",
    "\n",
    "        fog_index = 0.4 * ((total_words / total_sentences) + 100 * (complex_words / total_words))\n",
    "        return fog_index\n",
    "    \n",
    "    def calculate_sentiment_scores(self,text):\n",
    "        sentiment_scores = sid.polarity_scores(text)\n",
    "        return sentiment_scores\n",
    "    \n",
    "    def count_difficult_words(self, text, syllable_threshold=3):\n",
    "        words = TextBlob(text).words\n",
    "        difficult_words_count = sum(1 for word in words if self.count_syllables(word) >= syllable_threshold)\n",
    "        return difficult_words_count\n",
    "\n",
    "\n",
    "    \n",
    "    def run(self, \n",
    "            prompts: pd.DataFrame,\n",
    "            summaries:pd.DataFrame,\n",
    "            mode:str\n",
    "        ) -> pd.DataFrame:\n",
    "        \n",
    "        # before merge preprocess\n",
    "        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: len(word_tokenize(x))\n",
    "        )\n",
    "        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: word_tokenize(x)\n",
    "        )\n",
    "\n",
    "        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n",
    "            lambda x: len(word_tokenize(x))\n",
    "        )\n",
    "        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n",
    "            lambda x: word_tokenize(x)\n",
    "        )\n",
    "        \n",
    "        # Add prompt tokens into spelling checker dictionary\n",
    "        prompts[\"prompt_tokens\"].apply(\n",
    "            lambda x: self.add_spelling_dictionary(x)\n",
    "        )\n",
    "        \n",
    "        prompts['gunning_fog_prompt'] = prompts['prompt_text'].apply(self.gunning_fog)\n",
    "        prompts['flesch_kincaid_grade_level_prompt'] = prompts['prompt_text'].apply(self.flesch_kincaid_grade_level)\n",
    "        prompts['flesch_reading_ease_prompt'] = prompts['prompt_text'].apply(self.flesch_reading_ease_manual)\n",
    "      \n",
    "        \n",
    "        # count misspelling\n",
    "        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n",
    "        \n",
    "        # merge prompts and summaries\n",
    "        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n",
    "        input_df['flesch_reading_ease'] = input_df['text'].apply(self.flesch_reading_ease_manual)\n",
    "        input_df['word_count'] = input_df['text'].apply(lambda x: len(x.split()))\n",
    "        input_df['sentence_length'] = input_df['text'].apply(lambda x: len(x.split('.')))\n",
    "        input_df['vocabulary_richness'] = input_df['text'].apply(lambda x: len(set(x.split())))\n",
    "\n",
    "        input_df['word_count2'] = [len(t.split(' ')) for t in input_df.text]\n",
    "        input_df['num_unq_words']=[len(list(set(x.lower().split(' ')))) for x in input_df.text]\n",
    "        input_df['num_chars']= [len(x) for x in input_df.text]\n",
    "\n",
    "        # Additional features\n",
    "        input_df['avg_word_length'] = input_df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n",
    "        input_df['comma_count'] = input_df['text'].apply(lambda x: x.count(','))\n",
    "        input_df['semicolon_count'] = input_df['text'].apply(lambda x: x.count(';'))\n",
    "\n",
    "        # after merge preprocess\n",
    "        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n",
    "        \n",
    "        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n",
    "        input_df['bigram_overlap_count'] = input_df.progress_apply(\n",
    "            self.ngram_co_occurrence,args=(2,), axis=1 \n",
    "        )\n",
    "        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n",
    "        \n",
    "        input_df['trigram_overlap_count'] = input_df.progress_apply(\n",
    "            self.ngram_co_occurrence, args=(3,), axis=1\n",
    "        )\n",
    "        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n",
    "        \n",
    "        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n",
    "        \n",
    "        input_df['exclamation_count'] = input_df['text'].apply(lambda x: x.count('!'))\n",
    "        input_df['question_count'] = input_df['text'].apply(lambda x: x.count('?'))\n",
    "        input_df['pos_ratios'] = input_df['text'].apply(self.calculate_pos_ratios)\n",
    "\n",
    "        # Convert the dictionary of POS ratios into a single value (mean)\n",
    "        input_df['pos_mean'] = input_df['pos_ratios'].apply(lambda x: np.mean(list(x.values())))\n",
    "        input_df['punctuation_ratios'] = input_df['text'].apply(self.calculate_punctuation_ratios)\n",
    "\n",
    "        # Convert the dictionary of punctuation ratios into a single value (sum)\n",
    "        input_df['punctuation_sum'] = input_df['punctuation_ratios'].apply(lambda x: np.sum(list(x.values())))\n",
    "        input_df['keyword_density'] = input_df.apply(self.calculate_keyword_density, axis=1)\n",
    "        input_df['jaccard_similarity'] = input_df.apply(lambda row: len(set(word_tokenize(row['prompt_text'])) & set(word_tokenize(row['text']))) / len(set(word_tokenize(row['prompt_text'])) | set(word_tokenize(row['text']))), axis=1)\n",
    "        tqdm.pandas(desc=\"Performing Sentiment Analysis\")\n",
    "        input_df[['sentiment_polarity', 'sentiment_subjectivity']] = input_df['text'].progress_apply(\n",
    "            lambda x: pd.Series(self.sentiment_analysis(x))\n",
    "        )\n",
    "        tqdm.pandas(desc=\"Calculating Text Similarity\")\n",
    "        input_df['text_similarity'] = input_df.progress_apply(self.calculate_text_similarity, axis=1)\n",
    "        #Calculate sentiment scores for each row\n",
    "        input_df['sentiment_scores'] = input_df['text'].apply(self.calculate_sentiment_scores)\n",
    "        \n",
    "        input_df['gunning_fog'] = input_df['text'].apply(self.gunning_fog)\n",
    "        input_df['flesch_kincaid_grade_level'] = input_df['text'].apply(self.flesch_kincaid_grade_level)\n",
    "        input_df['count_difficult_words'] = input_df['text'].apply(self.count_difficult_words)\n",
    "\n",
    "        # Convert sentiment_scores into individual columns\n",
    "        sentiment_columns = pd.DataFrame(list(input_df['sentiment_scores']))\n",
    "        input_df = pd.concat([input_df, sentiment_columns], axis=1)\n",
    "        input_df['sentiment_scores_prompt'] = input_df['prompt_text'].apply(self.calculate_sentiment_scores)\n",
    "        # Convert sentiment_scores_prompt into individual columns\n",
    "        sentiment_columns_prompt = pd.DataFrame(list(input_df['sentiment_scores_prompt']))\n",
    "        sentiment_columns_prompt.columns = [col +'_prompt' for col in sentiment_columns_prompt.columns]\n",
    "        input_df = pd.concat([input_df, sentiment_columns_prompt], axis=1)\n",
    "        columns =  ['pos_ratios', 'sentiment_scores', 'punctuation_ratios', 'sentiment_scores_prompt']\n",
    "        cols_to_drop = [col for col in columns if col in input_df.columns]\n",
    "        if cols_to_drop:\n",
    "            input_df = input_df.drop(columns=cols_to_drop)\n",
    "        \n",
    "        print(cols_to_drop)\n",
    "        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n",
    "    \n",
    "preprocessor = Preprocessor(model_name=CFG.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe6e36f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:04:27.253674Z",
     "iopub.status.busy": "2023-10-30T22:04:27.253122Z",
     "iopub.status.idle": "2023-10-30T22:10:43.445244Z",
     "shell.execute_reply": "2023-10-30T22:10:43.444309Z"
    },
    "papermill": {
     "duration": 376.214696,
     "end_time": "2023-10-30T22:10:43.447299",
     "exception": false,
     "start_time": "2023-10-30T22:04:27.232603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7165/7165 [00:01<00:00, 4865.77it/s]\n",
      "100%|██████████| 7165/7165 [00:01<00:00, 6729.42it/s]\n",
      "100%|██████████| 7165/7165 [00:01<00:00, 3761.80it/s]\n",
      "100%|██████████| 7165/7165 [00:02<00:00, 3206.57it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 45581.69it/s]\n",
      "Performing Sentiment Analysis: 100%|██████████| 7165/7165 [00:07<00:00, 1017.90it/s]\n",
      "Calculating Text Similarity: 100%|██████████| 7165/7165 [00:29<00:00, 241.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos_ratios', 'sentiment_scores', 'punctuation_ratios', 'sentiment_scores_prompt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 8256.50it/s]\n",
      "Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 1470.14it/s]\n",
      "Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 2226.57it/s]\n",
      "Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 2091.66it/s]\n",
      "Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 1972.40it/s]\n",
      "Performing Sentiment Analysis: 100%|██████████| 4/4 [00:00<00:00, 1650.98it/s]\n",
      "Calculating Text Similarity: 100%|██████████| 4/4 [00:00<00:00, 296.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos_ratios', 'sentiment_scores', 'punctuation_ratios', 'sentiment_scores_prompt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n",
    "test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n",
    "\n",
    "# Calculate the number of unique groups\n",
    "n_unique_groups = train[\"grade\"].nunique()\n",
    "\n",
    "# Set n_splits to be the smaller of CFG.n_splits and the number of unique groups\n",
    "n_splits = min(CFG.n_splits, n_unique_groups)\n",
    "CFG.n_splits = n_splits\n",
    "\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"grade\"])):\n",
    "    train.loc[val_index, \"fold\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86246118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:43.554221Z",
     "iopub.status.busy": "2023-10-30T22:10:43.553962Z",
     "iopub.status.idle": "2023-10-30T22:10:43.588727Z",
     "shell.execute_reply": "2023-10-30T22:10:43.587859Z"
    },
    "papermill": {
     "duration": 0.090244,
     "end_time": "2023-10-30T22:10:43.590681",
     "exception": false,
     "start_time": "2023-10-30T22:10:43.500437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>...</th>\n",
       "      <th>count_difficult_words</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg_prompt</th>\n",
       "      <th>neu_prompt</th>\n",
       "      <th>pos_prompt</th>\n",
       "      <th>compound_prompt</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0020ae56ffbf</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>-0.548304</td>\n",
       "      <td>0.506755</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.4310</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.9949</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004e978e639e</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>3.128928</td>\n",
       "      <td>4.231226</td>\n",
       "      <td>269</td>\n",
       "      <td>32</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.9283</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005ab0199905</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.9283</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0070c9e7af47</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>3.272894</td>\n",
       "      <td>3.219757</td>\n",
       "      <td>232</td>\n",
       "      <td>29</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>ff7c7e70df07</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They used all sorts of chemical concoctions to...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.9949</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>ffc34d056498</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The lowest classes are slaves and farmers slav...</td>\n",
       "      <td>-0.308448</td>\n",
       "      <td>0.048171</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.6808</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.9283</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>ffd1576d2e1b</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>they sorta made people start workin...</td>\n",
       "      <td>-1.408180</td>\n",
       "      <td>-0.493603</td>\n",
       "      <td>51</td>\n",
       "      <td>10</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.9283</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>ffe4a98093b2</td>\n",
       "      <td>39c16e</td>\n",
       "      <td>An ideal tragety has three elements that make ...</td>\n",
       "      <td>-0.393310</td>\n",
       "      <td>0.627128</td>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.9969</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>fffbccfd8a08</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>The meat would smell sour but the would \"rub i...</td>\n",
       "      <td>1.771596</td>\n",
       "      <td>0.547742</td>\n",
       "      <td>114</td>\n",
       "      <td>9</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.7469</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.9949</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7165 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        student_id prompt_id  \\\n",
       "0     000e8c3c7ddb    814d6b   \n",
       "1     0020ae56ffbf    ebad26   \n",
       "2     004e978e639e    3b9047   \n",
       "3     005ab0199905    3b9047   \n",
       "4     0070c9e7af47    814d6b   \n",
       "...            ...       ...   \n",
       "7160  ff7c7e70df07    ebad26   \n",
       "7161  ffc34d056498    3b9047   \n",
       "7162  ffd1576d2e1b    3b9047   \n",
       "7163  ffe4a98093b2    39c16e   \n",
       "7164  fffbccfd8a08    ebad26   \n",
       "\n",
       "                                                   text   content   wording  \\\n",
       "0     The third wave was an experimentto see how peo...  0.205683  0.380538   \n",
       "1     They would rub it up with soda to make the sme... -0.548304  0.506755   \n",
       "2     In Egypt, there were many occupations and soci...  3.128928  4.231226   \n",
       "3     The highest class was Pharaohs these people we... -0.210614 -0.471415   \n",
       "4     The Third Wave developed  rapidly because the ...  3.272894  3.219757   \n",
       "...                                                 ...       ...       ...   \n",
       "7160  They used all sorts of chemical concoctions to...  0.205683  0.380538   \n",
       "7161  The lowest classes are slaves and farmers slav... -0.308448  0.048171   \n",
       "7162             they sorta made people start workin... -1.408180 -0.493603   \n",
       "7163  An ideal tragety has three elements that make ... -0.393310  0.627128   \n",
       "7164  The meat would smell sour but the would \"rub i...  1.771596  0.547742   \n",
       "\n",
       "      summary_length  splling_err_num  \\\n",
       "0                 64                5   \n",
       "1                 54                2   \n",
       "2                269               32   \n",
       "3                 28                5   \n",
       "4                232               29   \n",
       "...              ...              ...   \n",
       "7160              76                9   \n",
       "7161              49                7   \n",
       "7162              51               10   \n",
       "7163              63                5   \n",
       "7164             114                9   \n",
       "\n",
       "                                        prompt_question  \\\n",
       "0     Summarize how the Third Wave developed over su...   \n",
       "1     Summarize the various ways the factory would u...   \n",
       "2     In complete sentences, summarize the structure...   \n",
       "3     In complete sentences, summarize the structure...   \n",
       "4     Summarize how the Third Wave developed over su...   \n",
       "...                                                 ...   \n",
       "7160  Summarize the various ways the factory would u...   \n",
       "7161  In complete sentences, summarize the structure...   \n",
       "7162  In complete sentences, summarize the structure...   \n",
       "7163  Summarize at least 3 elements of an ideal trag...   \n",
       "7164  Summarize the various ways the factory would u...   \n",
       "\n",
       "                   prompt_title  \\\n",
       "0                The Third Wave   \n",
       "1       Excerpt from The Jungle   \n",
       "2     Egyptian Social Structure   \n",
       "3     Egyptian Social Structure   \n",
       "4                The Third Wave   \n",
       "...                         ...   \n",
       "7160    Excerpt from The Jungle   \n",
       "7161  Egyptian Social Structure   \n",
       "7162  Egyptian Social Structure   \n",
       "7163                 On Tragedy   \n",
       "7164    Excerpt from The Jungle   \n",
       "\n",
       "                                            prompt_text  ...  \\\n",
       "0     Background \\r\\nThe Third Wave experiment took ...  ...   \n",
       "1     With one member trimming beef in a cannery, an...  ...   \n",
       "2     Egyptian society was structured like a pyramid...  ...   \n",
       "3     Egyptian society was structured like a pyramid...  ...   \n",
       "4     Background \\r\\nThe Third Wave experiment took ...  ...   \n",
       "...                                                 ...  ...   \n",
       "7160  With one member trimming beef in a cannery, an...  ...   \n",
       "7161  Egyptian society was structured like a pyramid...  ...   \n",
       "7162  Egyptian society was structured like a pyramid...  ...   \n",
       "7163  Chapter 13 \\r\\nAs the sequel to what has alrea...  ...   \n",
       "7164  With one member trimming beef in a cannery, an...  ...   \n",
       "\n",
       "     count_difficult_words    neg    neu    pos compound  neg_prompt  \\\n",
       "0                        6  0.033  0.832  0.135   0.7845       0.027   \n",
       "1                        0  0.000  0.946  0.054   0.4310       0.086   \n",
       "2                       16  0.047  0.814  0.139   0.9725       0.063   \n",
       "3                        3  0.000  1.000  0.000   0.0000       0.063   \n",
       "4                       13  0.000  0.896  0.104   0.9696       0.027   \n",
       "...                    ...    ...    ...    ...      ...         ...   \n",
       "7160                     3  0.031  0.881  0.088   0.4601       0.086   \n",
       "7161                     2  0.167  0.784  0.049  -0.6808       0.063   \n",
       "7162                     2  0.000  0.939  0.061   0.4404       0.063   \n",
       "7163                     0  0.105  0.527  0.368   0.9715       0.229   \n",
       "7164                     3  0.000  0.939  0.061   0.7469       0.086   \n",
       "\n",
       "      neu_prompt  pos_prompt  compound_prompt  fold  \n",
       "0          0.873       0.100           0.9915   0.0  \n",
       "1          0.879       0.035          -0.9949   2.0  \n",
       "2          0.845       0.092           0.9283   1.0  \n",
       "3          0.845       0.092           0.9283   1.0  \n",
       "4          0.873       0.100           0.9915   0.0  \n",
       "...          ...         ...              ...   ...  \n",
       "7160       0.879       0.035          -0.9949   2.0  \n",
       "7161       0.845       0.092           0.9283   1.0  \n",
       "7162       0.845       0.092           0.9283   1.0  \n",
       "7163       0.628       0.143          -0.9969   0.0  \n",
       "7164       0.879       0.035          -0.9949   2.0  \n",
       "\n",
       "[7165 rows x 62 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca134c89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:43.697043Z",
     "iopub.status.busy": "2023-10-30T22:10:43.696820Z",
     "iopub.status.idle": "2023-10-30T22:10:43.720019Z",
     "shell.execute_reply": "2023-10-30T22:10:43.719207Z"
    },
    "papermill": {
     "duration": 0.078126,
     "end_time": "2023-10-30T22:10:43.721847",
     "exception": false,
     "start_time": "2023-10-30T22:10:43.643721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>...</th>\n",
       "      <th>flesch_kincaid_grade_level</th>\n",
       "      <th>count_difficult_words</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg_prompt</th>\n",
       "      <th>neu_prompt</th>\n",
       "      <th>pos_prompt</th>\n",
       "      <th>compound_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.313333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.313333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.313333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.313333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id            text  summary_length  splling_err_num  \\\n",
       "0  000000ffffff    abc123  Example text 1               3                0   \n",
       "1  111111eeeeee    def789  Example text 2               3                0   \n",
       "2  222222cccccc    abc123  Example text 3               3                0   \n",
       "3  333333dddddd    def789  Example text 4               3                0   \n",
       "\n",
       "  prompt_question     prompt_title       prompt_text title author  ...  \\\n",
       "0    Summarize...  Example Title 1  Heading\\nText...   NaN    NaN  ...   \n",
       "1    Summarize...  Example Title 2  Heading\\nText...   NaN    NaN  ...   \n",
       "2    Summarize...  Example Title 1  Heading\\nText...   NaN    NaN  ...   \n",
       "3    Summarize...  Example Title 2  Heading\\nText...   NaN    NaN  ...   \n",
       "\n",
       "  flesch_kincaid_grade_level count_difficult_words  neg  neu  pos  compound  \\\n",
       "0                   1.313333                     0  0.0  1.0  0.0       0.0   \n",
       "1                   1.313333                     0  0.0  1.0  0.0       0.0   \n",
       "2                   1.313333                     0  0.0  1.0  0.0       0.0   \n",
       "3                   1.313333                     0  0.0  1.0  0.0       0.0   \n",
       "\n",
       "   neg_prompt  neu_prompt  pos_prompt  compound_prompt  \n",
       "0         0.0         1.0         0.0              0.0  \n",
       "1         0.0         1.0         0.0              0.0  \n",
       "2         0.0         1.0         0.0              0.0  \n",
       "3         0.0         1.0         0.0              0.0  \n",
       "\n",
       "[4 rows x 59 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b531f0",
   "metadata": {
    "papermill": {
     "duration": 0.053496,
     "end_time": "2023-10-30T22:10:43.829393",
     "exception": false,
     "start_time": "2023-10-30T22:10:43.775897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a38d39c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:43.936891Z",
     "iopub.status.busy": "2023-10-30T22:10:43.936653Z",
     "iopub.status.idle": "2023-10-30T22:10:43.946834Z",
     "shell.execute_reply": "2023-10-30T22:10:43.946203Z"
    },
    "papermill": {
     "duration": 0.06656,
     "end_time": "2023-10-30T22:10:43.948718",
     "exception": false,
     "start_time": "2023-10-30T22:10:43.882158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    return {\"rmse\": rmse}\n",
    "\n",
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }\n",
    "\n",
    "def compt_score(content_true, content_pred, wording_true, wording_pred):\n",
    "    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n",
    "    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n",
    "    return (content_score + wording_score)/2\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True    \n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd53c3",
   "metadata": {
    "papermill": {
     "duration": 0.052572,
     "end_time": "2023-10-30T22:10:44.054945",
     "exception": false,
     "start_time": "2023-10-30T22:10:44.002373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bert-v3-large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9277820",
   "metadata": {
    "papermill": {
     "duration": 0.052735,
     "end_time": "2023-10-30T22:10:44.160423",
     "exception": false,
     "start_time": "2023-10-30T22:10:44.107688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This block contain loading of a pretrained model Berta-v3-large.\n",
    "\n",
    "link of the code of training model: https://www.kaggle.com/code/asteyagaur/commonlit-deberta-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41cbf146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:44.267362Z",
     "iopub.status.busy": "2023-10-30T22:10:44.266834Z",
     "iopub.status.idle": "2023-10-30T22:10:44.273457Z",
     "shell.execute_reply": "2023-10-30T22:10:44.272637Z"
    },
    "papermill": {
     "duration": 0.062189,
     "end_time": "2023-10-30T22:10:44.275337",
     "exception": false,
     "start_time": "2023-10-30T22:10:44.213148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:,i]#.detach().to('cpu').numpy()\n",
    "        y_pred = y_preds[:,i]#.detach().to('cpu').numpy()\n",
    "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def score_loss(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return {\n",
    "        'mcrmse_score' : mcrmse_score,\n",
    "        'Content_score' : scores[0],\n",
    "        'Wording_score' : scores[1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4e7c3ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:44.381819Z",
     "iopub.status.busy": "2023-10-30T22:10:44.381330Z",
     "iopub.status.idle": "2023-10-30T22:10:44.416111Z",
     "shell.execute_reply": "2023-10-30T22:10:44.415350Z"
    },
    "papermill": {
     "duration": 0.089878,
     "end_time": "2023-10-30T22:10:44.417912",
     "exception": false,
     "start_time": "2023-10-30T22:10:44.328034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7165, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>pred_content</th>\n",
       "      <th>pred_wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.658802</td>\n",
       "      <td>-0.093150</td>\n",
       "      <td>0.836136</td>\n",
       "      <td>0.151125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.638511</td>\n",
       "      <td>-0.911973</td>\n",
       "      <td>-1.300229</td>\n",
       "      <td>-0.716050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.205682</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>0.192420</td>\n",
       "      <td>0.089422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.355562</td>\n",
       "      <td>-0.955801</td>\n",
       "      <td>-1.039375</td>\n",
       "      <td>-0.897786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.205682</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>-0.006736</td>\n",
       "      <td>0.227909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    content   wording  pred_content  pred_wording\n",
       "0  1.658802 -0.093150      0.836136      0.151125\n",
       "1 -1.638511 -0.911973     -1.300229     -0.716050\n",
       "2  0.205682  0.380538      0.192420      0.089422\n",
       "3 -1.355562 -0.955801     -1.039375     -0.897786\n",
       "4  0.205682  0.380538     -0.006736      0.227909"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_bert_large = pd.read_csv('/kaggle/input/commonlit-deberta-v3-large-models/oof_df.csv')\n",
    "print(oof_bert_large.shape)\n",
    "oof_bert_large.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "626af01f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:44.527997Z",
     "iopub.status.busy": "2023-10-30T22:10:44.527199Z",
     "iopub.status.idle": "2023-10-30T22:10:44.536638Z",
     "shell.execute_reply": "2023-10-30T22:10:44.535934Z"
    },
    "papermill": {
     "duration": 0.067071,
     "end_time": "2023-10-30T22:10:44.538488",
     "exception": false,
     "start_time": "2023-10-30T22:10:44.471417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcrmse_score': 0.46615662019200665,\n",
       " 'Content_score': 0.39647407605816687,\n",
       " 'Wording_score': 0.5358391643258464}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = score_loss(np.array(oof_bert_large[['content' , 'wording']]) , np.array(oof_bert_large[['pred_content' , 'pred_wording']]))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edc6e3e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:44.645435Z",
     "iopub.status.busy": "2023-10-30T22:10:44.645235Z",
     "iopub.status.idle": "2023-10-30T22:10:45.038221Z",
     "shell.execute_reply": "2023-10-30T22:10:45.037350Z"
    },
    "papermill": {
     "duration": 0.449048,
     "end_time": "2023-10-30T22:10:45.040443",
     "exception": false,
     "start_time": "2023-10-30T22:10:44.591395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), '/kaggle/input/deberta-v3-large/deberta-v3-large')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class cfg:\n",
    "    select = 'large'\n",
    "    model_name = f'/kaggle/input/deberta-v3-{select}/deberta-v3-{select}'\n",
    "    only_model_name = f'deberta-v3-{select}'\n",
    "    fold = 4\n",
    "    batch_size = 32\n",
    "    freezing = True\n",
    "    max_len = 1024\n",
    "    pooling = 'GemText'\n",
    "    path = '/kaggle/input/commonlit-deberta-v3-large-models/'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    seed = 42\n",
    "cfg.tokenizer = AutoTokenizer.from_pretrained(cfg.path+'tokenizer/')\n",
    "cfg.device , cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "504d4a32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:45.149262Z",
     "iopub.status.busy": "2023-10-30T22:10:45.149067Z",
     "iopub.status.idle": "2023-10-30T22:10:45.160019Z",
     "shell.execute_reply": "2023-10-30T22:10:45.159233Z"
    },
    "papermill": {
     "duration": 0.067164,
     "end_time": "2023-10-30T22:10:45.161950",
     "exception": false,
     "start_time": "2023-10-30T22:10:45.094786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_logger(filename='Inference'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(cfg.seed)\n",
    "\n",
    "\n",
    "LOGGER.info(f\"max_len: {cfg.max_len}\")\n",
    "LOGGER.info(f\"batch_size: {cfg.batch_size}\")\n",
    "LOGGER.info(f\"Model name: {cfg.only_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b885f7e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:45.270949Z",
     "iopub.status.busy": "2023-10-30T22:10:45.270353Z",
     "iopub.status.idle": "2023-10-30T22:10:45.291012Z",
     "shell.execute_reply": "2023-10-30T22:10:45.290161Z"
    },
    "papermill": {
     "duration": 0.07767,
     "end_time": "2023-10-30T22:10:45.292877",
     "exception": false,
     "start_time": "2023-10-30T22:10:45.215207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>student_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abc123</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>Example text 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abc123</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>Example text 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def789</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>Example text 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def789</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>Example text 4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_id prompt_question     prompt_title       prompt_text    student_id  \\\n",
       "0    abc123    Summarize...  Example Title 1  Heading\\nText...  000000ffffff   \n",
       "1    abc123    Summarize...  Example Title 1  Heading\\nText...  222222cccccc   \n",
       "2    def789    Summarize...  Example Title 2  Heading\\nText...  111111eeeeee   \n",
       "3    def789    Summarize...  Example Title 2  Heading\\nText...  333333dddddd   \n",
       "\n",
       "             text  \n",
       "0  Example text 1  \n",
       "1  Example text 3  \n",
       "2  Example text 2  \n",
       "3  Example text 4  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_test1 = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\n",
    "summary_test1 = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\n",
    "submission1 = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv')\n",
    "\n",
    "test1 = prompts_test1.merge(summary_test1, on=\"prompt_id\")\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e907cc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:45.402226Z",
     "iopub.status.busy": "2023-10-30T22:10:45.401804Z",
     "iopub.status.idle": "2023-10-30T22:10:45.409680Z",
     "shell.execute_reply": "2023-10-30T22:10:45.408892Z"
    },
    "papermill": {
     "duration": 0.064399,
     "end_time": "2023-10-30T22:10:45.411517",
     "exception": false,
     "start_time": "2023-10-30T22:10:45.347118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Summarize... [SEP] Example text 1\n",
       "1    Summarize... [SEP] Example text 3\n",
       "2    Summarize... [SEP] Example text 2\n",
       "3    Summarize... [SEP] Example text 4\n",
       "Name: full_text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1['full_text']=test1['prompt_question'] +\" \" + cfg.tokenizer.sep_token +\" \"+test1['text']\n",
    "test1['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ca5c616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:45.521356Z",
     "iopub.status.busy": "2023-10-30T22:10:45.521152Z",
     "iopub.status.idle": "2023-10-30T22:10:45.528311Z",
     "shell.execute_reply": "2023-10-30T22:10:45.527621Z"
    },
    "papermill": {
     "duration": 0.064935,
     "end_time": "2023-10-30T22:10:45.530110",
     "exception": false,
     "start_time": "2023-10-30T22:10:45.465175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def odd_layer_freeze(module):\n",
    "    for i in range(1,24,2):\n",
    "        for n,p in module.encoder.layer[i].named_parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "def even_layer_freeze(module):\n",
    "    for i in range(0,24,2):\n",
    "        for n,p in module.encoder.layer[i].named_parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "def top_half_layer_freeze(module):\n",
    "    for i in range(0,13,1):\n",
    "        for n,p in module.encoder.layer[i].named_parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "def bottom_half_layer_freeze(module):\n",
    "    for i in range(13,14,1):\n",
    "        for n,p in module.encoder.layer[i].named_parameters():\n",
    "            p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebeb15a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:45.638957Z",
     "iopub.status.busy": "2023-10-30T22:10:45.638753Z",
     "iopub.status.idle": "2023-10-30T22:10:45.673936Z",
     "shell.execute_reply": "2023-10-30T22:10:45.673004Z"
    },
    "papermill": {
     "duration": 0.092056,
     "end_time": "2023-10-30T22:10:45.675763",
     "exception": false,
     "start_time": "2023-10-30T22:10:45.583707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeMText()\n"
     ]
    }
   ],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "    \n",
    "class GeMText(nn.Module):\n",
    "    def __init__(self, dim = 1, p=3, eps=1e-6):\n",
    "        super(GeMText, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.feat_mult = 1\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n",
    "        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n",
    "        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n",
    "        ret = ret.pow(1 / self.p)\n",
    "        return ret\n",
    "\n",
    "\n",
    "\n",
    "def get_pooling_layer():\n",
    "    if cfg.pooling == 'Mean':\n",
    "        return MeanPooling()\n",
    "    \n",
    "    elif cfg.pooling == 'Max':\n",
    "        return MaxPooling()\n",
    "    \n",
    "    elif cfg.pooling == 'MeanMax':\n",
    "        return MeanMaxPooling()\n",
    "    \n",
    "    elif cfg.pooling == 'GemText':\n",
    "        return GeMText()\n",
    "\n",
    "\n",
    "print(get_pooling_layer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2378c11d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:45.784954Z",
     "iopub.status.busy": "2023-10-30T22:10:45.784262Z",
     "iopub.status.idle": "2023-10-30T22:10:45.794909Z",
     "shell.execute_reply": "2023-10-30T22:10:45.794051Z"
    },
    "papermill": {
     "duration": 0.067071,
     "end_time": "2023-10-30T22:10:45.796777",
     "exception": false,
     "start_time": "2023-10-30T22:10:45.729706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaselineModel1(nn.Module):\n",
    "    def __init__(self, model_name ):\n",
    "        super(BaselineModel1, self).__init__()\n",
    "        \n",
    "        self.model = AutoModel.from_pretrained(cfg.model_name)\n",
    "        self.config = AutoConfig.from_pretrained(cfg.model_name)\n",
    "        self.pooler = get_pooling_layer()\n",
    "\n",
    "        if cfg.pooling == 'MeanMax':\n",
    "            self.fc = nn.Linear(2*self.config.hidden_size, 2)\n",
    "        else:\n",
    "            self.fc = nn.Linear(self.config.hidden_size, 2)\n",
    "            \n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "        if cfg.freezing:\n",
    "            top_half_layer_freeze(self.model)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "           \n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids,attention_mask=mask,\n",
    "                         output_hidden_states=False)\n",
    "        out = self.pooler(out.last_hidden_state, mask)\n",
    "        outputs = self.fc(out)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab5393e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:45.905768Z",
     "iopub.status.busy": "2023-10-30T22:10:45.905183Z",
     "iopub.status.idle": "2023-10-30T22:10:45.914437Z",
     "shell.execute_reply": "2023-10-30T22:10:45.913569Z"
    },
    "papermill": {
     "duration": 0.066211,
     "end_time": "2023-10-30T22:10:45.916451",
     "exception": false,
     "start_time": "2023-10-30T22:10:45.850240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        self.tokenizer = cfg.tokenizer\n",
    "        self.max_len = cfg.max_len\n",
    "        self.pq = df['prompt_question'].values\n",
    "        self.text = df['text'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self , index):\n",
    "        pq   =   self.pq[index]\n",
    "        text =   self.text[index]\n",
    "        full_text = pq+\" \" + self.tokenizer.sep_token +\" \"+text\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "                        full_text,\n",
    "                        truncation=True,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=self.max_len,\n",
    "                        padding='max_length'\n",
    "                        \n",
    "                    )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:, :mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bc5c6a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:46.026431Z",
     "iopub.status.busy": "2023-10-30T22:10:46.026220Z",
     "iopub.status.idle": "2023-10-30T22:10:46.032711Z",
     "shell.execute_reply": "2023-10-30T22:10:46.031878Z"
    },
    "papermill": {
     "duration": 0.063561,
     "end_time": "2023-10-30T22:10:46.034539",
     "exception": false,
     "start_time": "2023-10-30T22:10:45.970978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_run(model , loader):   \n",
    "    model.eval()\n",
    "       \n",
    "    preds = []\n",
    "    bar = tqdm(enumerate(loader), total=len(loader))\n",
    "    for idx , data in bar:\n",
    "        inputs = collate(data)\n",
    "        ids   =  inputs['input_ids'].to(cfg.device, dtype = torch.long)\n",
    "        mask  =  inputs['attention_mask'].to(cfg.device, dtype = torch.long)\n",
    "        y_preds = model(ids , mask)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    \n",
    "    predictions = np.concatenate(preds)\n",
    "    \n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e985d3bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:46.144200Z",
     "iopub.status.busy": "2023-10-30T22:10:46.143961Z",
     "iopub.status.idle": "2023-10-30T22:10:46.148612Z",
     "shell.execute_reply": "2023-10-30T22:10:46.147805Z"
    },
    "papermill": {
     "duration": 0.061979,
     "end_time": "2023-10-30T22:10:46.150461",
     "exception": false,
     "start_time": "2023-10-30T22:10:46.088482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(test1)\n",
    "test_loader = DataLoader(test_dataset , batch_size=cfg.batch_size ,num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3382c3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:10:46.259827Z",
     "iopub.status.busy": "2023-10-30T22:10:46.259121Z",
     "iopub.status.idle": "2023-10-30T22:12:43.042746Z",
     "shell.execute_reply": "2023-10-30T22:12:43.041747Z"
    },
    "papermill": {
     "duration": 116.841031,
     "end_time": "2023-10-30T22:12:43.045383",
     "exception": false,
     "start_time": "2023-10-30T22:10:46.204352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** fold 0 ********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** fold 1 ********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** fold 2 ********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** fold 3 ********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.99it/s]\n"
     ]
    }
   ],
   "source": [
    "final_preds = []\n",
    "for fold in range(cfg.fold):\n",
    "    print('******** fold' , fold , '********')\n",
    "    \n",
    "    model  = BaselineModel1(cfg.model_name).to(cfg.device)\n",
    "    model.load_state_dict(torch.load(f\"/kaggle/input/commonlit-deberta-v3-large-models/deberta-v3-{cfg.select}_Fold_{fold}.pth\", map_location=torch.device('cpu')))\n",
    "    preds = test_run(model, test_loader)\n",
    "    final_preds.append(preds)\n",
    "    del model ; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "final_preds_ = np.mean(final_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "390cabe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:12:43.173948Z",
     "iopub.status.busy": "2023-10-30T22:12:43.173644Z",
     "iopub.status.idle": "2023-10-30T22:12:43.181394Z",
     "shell.execute_reply": "2023-10-30T22:12:43.180354Z"
    },
    "papermill": {
     "duration": 0.075332,
     "end_time": "2023-10-30T22:12:43.183559",
     "exception": false,
     "start_time": "2023-10-30T22:12:43.108227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.409746 , -1.039957 ],\n",
       "       [-1.4451523, -1.0264542],\n",
       "       [-1.4189626, -1.0212438],\n",
       "       [-1.4166303, -1.0149112]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are interested on\n",
    "final_preds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "264c481c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:12:43.311876Z",
     "iopub.status.busy": "2023-10-30T22:12:43.311611Z",
     "iopub.status.idle": "2023-10-30T22:12:43.317266Z",
     "shell.execute_reply": "2023-10-30T22:12:43.316333Z"
    },
    "papermill": {
     "duration": 0.072252,
     "end_time": "2023-10-30T22:12:43.319446",
     "exception": false,
     "start_time": "2023-10-30T22:12:43.247194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_preds_test_bert_large = pd.DataFrame(data=final_preds_, columns=[\"content_pred_debertav3large\", \"wording_pred_debertav3large\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a384421",
   "metadata": {
    "papermill": {
     "duration": 0.063883,
     "end_time": "2023-10-30T22:12:43.447522",
     "exception": false,
     "start_time": "2023-10-30T22:12:43.383639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bert-v3-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47ab4e",
   "metadata": {
    "papermill": {
     "duration": 0.062326,
     "end_time": "2023-10-30T22:12:43.573442",
     "exception": false,
     "start_time": "2023-10-30T22:12:43.511116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This code contain training of custom architecture of Bert-base model. \n",
    "Сustom architecture define Poolings, regression head, number of layers to freeze. \n",
    "\n",
    "The model is defined through the CFG class, all necessary parameters: path to weights, pooling type, sequence length and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e9afc32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:12:43.708973Z",
     "iopub.status.busy": "2023-10-30T22:12:43.708126Z",
     "iopub.status.idle": "2023-10-30T22:12:43.723373Z",
     "shell.execute_reply": "2023-10-30T22:12:43.722621Z"
    },
    "papermill": {
     "duration": 0.085449,
     "end_time": "2023-10-30T22:12:43.725741",
     "exception": false,
     "start_time": "2023-10-30T22:12:43.640292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, disable_progress_bar\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob\n",
    "import optuna\n",
    "import optuna.integration.lightgbm as lgb\n",
    "import pyphen\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import Counter\n",
    "from nltk import ne_chunk, word_tokenize, pos_tag\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "from spellchecker import SpellChecker\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "disable_progress_bar()\n",
    "tqdm.pandas()\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1bee9b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:12:43.857511Z",
     "iopub.status.busy": "2023-10-30T22:12:43.857261Z",
     "iopub.status.idle": "2023-10-30T22:12:43.875330Z",
     "shell.execute_reply": "2023-10-30T22:12:43.874268Z"
    },
    "papermill": {
     "duration": 0.085642,
     "end_time": "2023-10-30T22:12:43.877370",
     "exception": false,
     "start_time": "2023-10-30T22:12:43.791728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from datasets import Dataset\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "    \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        # using unsqueeze and expand, the attention mask is expanded to the same size as last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "    \n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        embeddings = last_hidden_state.clone()\n",
    "        embeddings[input_mask_expanded == 0] = -1e9\n",
    "        max_embeddings, _ = torch.max(embeddings, dim = 1)\n",
    "        return max_embeddings\n",
    "      \n",
    "class ClsPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClsPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        return last_hidden_state[:,0,:]\n",
    "    \n",
    "class MeanMax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanMax, self).__init__()\n",
    "        \n",
    "        self.mean_pooler = MeanPooling()\n",
    "        self.max_pooler  = MaxPooling()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        mean_pooler = self.mean_pooler( last_hidden_state ,attention_mask )\n",
    "        max_pooler =  self.max_pooler( last_hidden_state ,attention_mask )\n",
    "        out = torch.concat([mean_pooler ,max_pooler ] , 1)\n",
    "        return out\n",
    "\n",
    "class GeMText(nn.Module):\n",
    "    def __init__(self, dim = 1, p=3, eps=1e-6):\n",
    "        super(GeMText, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.feat_mult = 1\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n",
    "        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n",
    "        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n",
    "        ret = ret.pow(1 / self.p)\n",
    "        return ret\n",
    "    \n",
    "def get_pooling_layer(pooling):\n",
    "    if pooling == 'Mean':\n",
    "        return MeanPooling()\n",
    "    \n",
    "    elif pooling == 'Max':\n",
    "        return MaxPooling()\n",
    "    \n",
    "    elif pooling == 'MeanMax':\n",
    "        return MeanMax()\n",
    "    \n",
    "    elif pooling == 'GemText':\n",
    "        return GeMText()\n",
    "    \n",
    "    elif pooling == 'ClsPooling':\n",
    "        return ClsPooling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3131c825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:12:44.006106Z",
     "iopub.status.busy": "2023-10-30T22:12:44.005832Z",
     "iopub.status.idle": "2023-10-30T22:12:44.023956Z",
     "shell.execute_reply": "2023-10-30T22:12:44.023088Z"
    },
    "papermill": {
     "duration": 0.084389,
     "end_time": "2023-10-30T22:12:44.026189",
     "exception": false,
     "start_time": "2023-10-30T22:12:43.941800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput \n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, config, model_name=\"bert\", ft=True):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        if ft:\n",
    "            self.model = AutoModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(config)\n",
    "        self.config = config\n",
    "        self.problem_type = \"regression\"\n",
    "        self.num_labels = 2\n",
    "        self.pooler = get_pooling_layer(config.pooling)\n",
    "            \n",
    "        if config.pooling == 'MeanMax':\n",
    "            # creates a fully connected linear layer nn.Linear with input space size, \n",
    "            # equal to twice the value of self.config.hidden_size, \n",
    "            # and output space size equal to 2.\n",
    "            self.fc = nn.Linear(2*self.config.hidden_size, 2)\n",
    "        else:\n",
    "            self.fc = nn.Linear(self.config.hidden_size, 2)\n",
    "    \n",
    "    # these are the input parameters\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None, \n",
    "        position_ids=None,        \n",
    "        head_mask=None,        \n",
    "        inputs_embeds=None,       \n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=False,\n",
    "\n",
    "        return_dict=True):\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        #  self.pooler - combines embeddings to get a fixed size view\n",
    "        out = self.pooler(outputs.last_hidden_state, attention_mask)\n",
    "        # !!!! linear layer to produce finite output logits\n",
    "        logits = self.fc(out)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "                \n",
    "        # if return_dict is False, the tuple (logits,) with added values from outputs starting at index 2 is returned    \n",
    "        # if loss is not None, the tuple ((\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "662b9cb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:12:44.152845Z",
     "iopub.status.busy": "2023-10-30T22:12:44.152554Z",
     "iopub.status.idle": "2023-10-30T22:12:44.206231Z",
     "shell.execute_reply": "2023-10-30T22:12:44.205321Z"
    },
    "papermill": {
     "duration": 0.116847,
     "end_time": "2023-10-30T22:12:44.208306",
     "exception": false,
     "start_time": "2023-10-30T22:12:44.091459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_n_infer(train,\n",
    "                  val,\n",
    "                  model_name=\"roberta-base\",\n",
    "                  preproc_type=1,\n",
    "                  batch_size=4,\n",
    "                  learning_rate=5e-5,\n",
    "                  warmup_ratio=0,\n",
    "                  weight_decay=0.01,\n",
    "                  hidden_dropout_prob=0.0,\n",
    "                  attention_probs_dropout_prob=0.0,\n",
    "                  num_layers_to_freeze=0,\n",
    "                  num_train_epochs=2,\n",
    "                  save_steps=100,\n",
    "                  random_seed=42,\n",
    "                  max_length=512,\n",
    "                  model_dir=\"bert\"):\n",
    "    \n",
    "    \n",
    "    # select the fields we need from the dataset\n",
    "    train_content = train[[\"text\", \"prompt_question\", \"prompt_text\", 'prompt_title',\"content\", \"wording\"]]\n",
    "    val_content = val[[\"text\", \"prompt_question\", \"prompt_text\", 'prompt_title', \"content\", \"wording\"]]\n",
    "    test_ = test[[\"text\", \"prompt_question\", \"prompt_text\", 'prompt_title']]\n",
    "    \n",
    "    \n",
    "    # import tokenizer and model config. We change the config for the regression task, since we will use Trainer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    \n",
    "    # these config parameters are used by Trainer to tune the model\n",
    "    model_config.update({\n",
    "        \"pooling\": CFG.pooling,\n",
    "        'num_labels': 2,\n",
    "        'problem_type': 'regression'})\n",
    "        \n",
    "    seed_everything(seed=42) \n",
    "    # import model\n",
    "    model_content = AutoModelForSequenceClassification.from_pretrained(model_name, config=model_config)\n",
    "    # customized realization of model\n",
    "    model_content = BaselineModel(config=model_config, model_name=model_name) \n",
    "    \n",
    "    # freezing layers\n",
    "    for name, param in list(model_content.named_parameters())[: num_layers_to_freeze]:\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # import dataset from pandas to intermediate format\n",
    "    train_dataset_content = Dataset.from_pandas(train_content, preserve_index=False) \n",
    "    val_dataset_content = Dataset.from_pandas(val_content, preserve_index=False) \n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        # tokenizer has just been realized\n",
    "        sep_token = tokenizer.sep_token\n",
    "        labels = [examples[\"content\"], examples[\"wording\"]]\n",
    "        \n",
    "        if preproc_type == 1:\n",
    "            example = examples[\"text\"]\n",
    "        elif preproc_type == 2:\n",
    "            example = f\"{examples['prompt_question']} {sep_token} {examples['text']}\"\n",
    "        elif preproc_type == 3:\n",
    "            example = f\"{examples['prompt_question']} {sep_token} {examples['text']}{sep_token} {examples['prompt_text']}\"\n",
    "        \n",
    "        elif preproc_type == 4:\n",
    "            example = f\"{examples['prompt_title']} {sep_token} {examples['prompt_question']} {sep_token} {examples['text']}\"\n",
    "        elif preproc_type == 5:\n",
    "            example = f\"{examples['prompt_question']} {sep_token} {examples['prompt_title']}{sep_token} {examples['text']}\"\n",
    "        elif preproc_type == 6:\n",
    "            example = f\"{examples['text']} {sep_token} {examples['prompt_text ']}\"\n",
    "\n",
    "        tokenized = tokenizer(example,\n",
    "                         padding=False,\n",
    "                         truncation=True,\n",
    "                         max_length=max_length)       \n",
    "           \n",
    "    \n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"labels\": labels,       \n",
    "        }\n",
    "    \n",
    "    #  tokenization of text and test\n",
    "    def tokenize_function_test(examples):\n",
    "        sep_token = tokenizer.sep_token\n",
    " \n",
    "        if preproc_type == 1:\n",
    "            example = examples[\"text\"]\n",
    "        elif preproc_type == 2:\n",
    "            example = f\"{examples['prompt_question']} {sep_token} {examples['text']}\"\n",
    "        elif preproc_type == 3:\n",
    "            example = f\"{examples['prompt_question']} {sep_token} {examples['text']} {sep_token} {examples['prompt_text']}\"\n",
    "        elif preproc_type == 4:\n",
    "            example = f\"{examples['prompt_title']} {sep_token} {examples['prompt_question']} {sep_token} {examples['text']}\"\n",
    "        elif preproc_type == 5:\n",
    "            example = f\"{examples['prompt_question']} {sep_token} {examples['prompt_title']}{sep_token} {examples['text']}\"\n",
    "        elif preproc_type == 6:\n",
    "            example = f\"{examples['text']} {sep_token} {examples['prompt_text ']}\"\n",
    "        tokenized = tokenizer(example,\n",
    "                         padding=False,        \n",
    "                         truncation=True,     \n",
    "                         max_length=max_length)\n",
    "        return tokenized\n",
    "    \n",
    "    # Tokenize train, so we don't have to do it during training.\n",
    "    # this is the dataset to which the tokenizer has been applied\n",
    "    train_tokenized_datasets_content = train_dataset_content.map(tokenize_function, batched=False)\n",
    "    val_tokenized_datasets_content = val_dataset_content.map(tokenize_function, batched=False)\n",
    "  \n",
    "    # Tokenize test\n",
    "    test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n",
    "    test_tokenized_dataset = test_dataset.map(tokenize_function_test, batched=False)\n",
    "    \n",
    "    # create iterator with tokenizer\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    model_dir = model_dir\n",
    "    \n",
    "    # initialize trainer arguments , giving  model_dir and other data from config\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_dir,\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        report_to='none',\n",
    "        greater_is_better=False,\n",
    "        save_strategy=\"steps\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=save_steps,\n",
    "        save_steps=save_steps,\n",
    "        metric_for_best_model=\"mcrmse\",\n",
    "        save_total_limit=1\n",
    "    )\n",
    "    # MAKING OF TRAINER\n",
    "    #  IMPORTANT - model_content(has been initialized before and contain CFG) is placed into trainer_content \n",
    "    trainer_content = Trainer(\n",
    "        model=model_content,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized_datasets_content,\n",
    "        eval_dataset=val_tokenized_datasets_content,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_mcrmse,\n",
    "        data_collator=data_collator)\n",
    "   \n",
    "    # STARTING OF TRAINING\n",
    "    trainer_content.train()\n",
    "    best_check = os.listdir(model_dir)[0]\n",
    "    \n",
    "    # make inference to test (model_content)\n",
    "    model_content = BaselineModel(config=model_config, ft=False)\n",
    "    model_content.load_state_dict(torch.load(f\"{model_dir}/{best_check}/pytorch_model.bin\"))\n",
    "    model_content.eval()\n",
    "\n",
    "    # sets the configuration for model testing\n",
    "    test_args = TrainingArguments(\n",
    "        output_dir=model_dir,\n",
    "        # False means that model testing will not be followed by training\n",
    "        do_train = False,\n",
    "        do_predict = True,\n",
    "        # batch size for model evaluation on each device\n",
    "        per_device_eval_batch_size = 4,  \n",
    "        # False means that the last incomplete data packet will not be discarded when estimating the model.\n",
    "        dataloader_drop_last = False,\n",
    "    )\n",
    "    # init trainer\n",
    "    infer_content = Trainer(\n",
    "                  model = model_content, \n",
    "                  tokenizer=tokenizer,\n",
    "                  data_collator=data_collator,\n",
    "                  args = test_args)\n",
    "    # TRAINED MODEL  PREDICTS TEST \n",
    "    val_results_content = infer_content.predict(val_tokenized_datasets_content)[0]\n",
    "    test_results_content = infer_content.predict(test_tokenized_dataset)[0]\n",
    "    \n",
    "    # save model\n",
    "    torch.save(model_content.state_dict(), f\"{model_dir}/pytorch_model.bin\")\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "    with open(f\"{model_dir}/config.json\", \"w\") as file:\n",
    "        json.dump(model_config.to_dict(), file)\n",
    "    \n",
    "    shutil.rmtree(f\"{model_dir}/{best_check}\")\n",
    "\n",
    "    return val_results_content, test_results_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99b3d915",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:12:44.325459Z",
     "iopub.status.busy": "2023-10-30T22:12:44.325168Z",
     "iopub.status.idle": "2023-10-30T22:12:44.348350Z",
     "shell.execute_reply": "2023-10-30T22:12:44.347481Z"
    },
    "papermill": {
     "duration": 0.083828,
     "end_time": "2023-10-30T22:12:44.350434",
     "exception": false,
     "start_time": "2023-10-30T22:12:44.266606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>...</th>\n",
       "      <th>count_difficult_words</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg_prompt</th>\n",
       "      <th>neu_prompt</th>\n",
       "      <th>pos_prompt</th>\n",
       "      <th>compound_prompt</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id                                               text  \\\n",
       "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
       "\n",
       "    content   wording  summary_length  splling_err_num  \\\n",
       "0  0.205683  0.380538              64                5   \n",
       "\n",
       "                                     prompt_question    prompt_title  \\\n",
       "0  Summarize how the Third Wave developed over su...  The Third Wave   \n",
       "\n",
       "                                         prompt_text  ...  \\\n",
       "0  Background \\r\\nThe Third Wave experiment took ...  ...   \n",
       "\n",
       "  count_difficult_words    neg    neu    pos compound  neg_prompt  neu_prompt  \\\n",
       "0                     6  0.033  0.832  0.135   0.7845       0.027       0.873   \n",
       "\n",
       "   pos_prompt  compound_prompt  fold  \n",
       "0         0.1           0.9915   0.0  \n",
       "\n",
       "[1 rows x 62 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3272c954",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:12:44.469557Z",
     "iopub.status.busy": "2023-10-30T22:12:44.468958Z",
     "iopub.status.idle": "2023-10-30T22:12:44.494257Z",
     "shell.execute_reply": "2023-10-30T22:12:44.493398Z"
    },
    "papermill": {
     "duration": 0.084352,
     "end_time": "2023-10-30T22:12:44.496249",
     "exception": false,
     "start_time": "2023-10-30T22:12:44.411897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>...</th>\n",
       "      <th>flesch_kincaid_grade_level</th>\n",
       "      <th>count_difficult_words</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg_prompt</th>\n",
       "      <th>neu_prompt</th>\n",
       "      <th>pos_prompt</th>\n",
       "      <th>compound_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.313333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.313333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.313333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.313333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id            text  summary_length  splling_err_num  \\\n",
       "0  000000ffffff    abc123  Example text 1               3                0   \n",
       "1  111111eeeeee    def789  Example text 2               3                0   \n",
       "2  222222cccccc    abc123  Example text 3               3                0   \n",
       "3  333333dddddd    def789  Example text 4               3                0   \n",
       "\n",
       "  prompt_question     prompt_title       prompt_text title author  ...  \\\n",
       "0    Summarize...  Example Title 1  Heading\\nText...   NaN    NaN  ...   \n",
       "1    Summarize...  Example Title 2  Heading\\nText...   NaN    NaN  ...   \n",
       "2    Summarize...  Example Title 1  Heading\\nText...   NaN    NaN  ...   \n",
       "3    Summarize...  Example Title 2  Heading\\nText...   NaN    NaN  ...   \n",
       "\n",
       "  flesch_kincaid_grade_level count_difficult_words  neg  neu  pos  compound  \\\n",
       "0                   1.313333                     0  0.0  1.0  0.0       0.0   \n",
       "1                   1.313333                     0  0.0  1.0  0.0       0.0   \n",
       "2                   1.313333                     0  0.0  1.0  0.0       0.0   \n",
       "3                   1.313333                     0  0.0  1.0  0.0       0.0   \n",
       "\n",
       "   neg_prompt  neu_prompt  pos_prompt  compound_prompt  \n",
       "0         0.0         1.0         0.0              0.0  \n",
       "1         0.0         1.0         0.0              0.0  \n",
       "2         0.0         1.0         0.0              0.0  \n",
       "3         0.0         1.0         0.0              0.0  \n",
       "\n",
       "[4 rows x 59 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "791c01b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:12:44.612015Z",
     "iopub.status.busy": "2023-10-30T22:12:44.611289Z",
     "iopub.status.idle": "2023-10-30T22:12:44.625340Z",
     "shell.execute_reply": "2023-10-30T22:12:44.624455Z"
    },
    "papermill": {
     "duration": 0.074596,
     "end_time": "2023-10-30T22:12:44.627227",
     "exception": false,
     "start_time": "2023-10-30T22:12:44.552631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_oof_pred_n_test(train,\n",
    "                        model_name=\"/kaggle/input/berttiny\",\n",
    "                        preproc_type=1,\n",
    "                        n_splits=3,\n",
    "                        batch_size=4,\n",
    "                        learning_rate=5e-5,\n",
    "                        warmup_ratio=0,\n",
    "                        hidden_dropout_prob=0.0,\n",
    "                        attention_probs_dropout_prob=0.0,\n",
    "                        num_layers_to_freeze=0,\n",
    "                        weight_decay=0.01,\n",
    "                        num_train_epochs=2,\n",
    "                        random_seed=42,\n",
    "                        save_steps=100,\n",
    "                        max_length=512\n",
    "                       ):\n",
    "    kfl = GroupKFold(n_splits=n_splits)\n",
    "    oof_content = np.zeros((len(train), 2))\n",
    "    test_pred_content = np.zeros((len(test), 2))\n",
    "\n",
    "    model_name_ = model_name.split(\"/\")[-1]\n",
    "    if os.path.exists(model_name_):\n",
    "        shutil.rmtree(model_name_)\n",
    "    os.mkdir(model_name_)\n",
    "    \n",
    "    pred_dict=[]\n",
    "    for i, (train_indx, val_indx) in enumerate(kfl.split(train, groups=train[\"prompt_id\"])):\n",
    "        print(f\"fold {i}:\")\n",
    "        \n",
    "        train_ = train.iloc[train_indx]\n",
    "        val_ = train.iloc[val_indx]\n",
    "        val_res_content, test_res_content  = train_n_infer(train_,\n",
    "                                                           val_,\n",
    "                                                           model_name=model_name,\n",
    "                                                           preproc_type=preproc_type,\n",
    "                                                           batch_size=batch_size,\n",
    "                                                           learning_rate=learning_rate,\n",
    "                                                           warmup_ratio=warmup_ratio,\n",
    "                                                           hidden_dropout_prob=hidden_dropout_prob,\n",
    "                                                           attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "                                                           num_layers_to_freeze=num_layers_to_freeze,\n",
    "                                                           weight_decay=weight_decay,\n",
    "                                                           num_train_epochs=num_train_epochs,\n",
    "                                                           save_steps=save_steps,\n",
    "                                                           max_length=max_length,\n",
    "                                                           random_seed=random_seed,\n",
    "                                                           model_dir=f\"{model_name_}/fold_{i}\"\n",
    "                                                           )\n",
    "\n",
    "        oof_content[val_indx] += val_res_content\n",
    "        test_pred_content = test_pred_content+test_res_content/n_splits\n",
    "        \n",
    "        pred_dict.append(test_pred_content)       \n",
    "    \n",
    "    oof_train = pd.DataFrame(oof_content, columns=[f\"content_pred_{model_name_}\", f\"wording_pred_{model_name_}\"])\n",
    "    test_pred = pd.DataFrame(test_pred_content, columns=[f\"content_pred_{model_name_}\", f\"wording_pred_{model_name_}\"])\n",
    "    \n",
    "    display('test_pred',test_pred )\n",
    "    \n",
    "    train.loc[:, [f\"content_pred_{model_name_}\", f\"wording_pred_{model_name_}\" ] ] = oof_content  \n",
    "    \n",
    "    cv_metric = compute_mcrmse((oof_train.values, train[[\"content\", \"wording\"]]))\n",
    "    cv_metric[\"preproc_type\"] = preproc_type\n",
    "    print(f\"cv mcrmse: {cv_metric}\")\n",
    "    with open(f\"{model_name_}/cv_metric.json\", \"w\") as outfile:\n",
    "        json.dump(cv_metric, outfile)\n",
    "    oof_train.to_csv(f\"{model_name_}/oof_train.csv\", index=False)\n",
    "    \n",
    "    return oof_train, pred_dict, test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9085dc5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T22:12:44.742378Z",
     "iopub.status.busy": "2023-10-30T22:12:44.741757Z",
     "iopub.status.idle": "2023-10-30T23:29:07.792330Z",
     "shell.execute_reply": "2023-10-30T23:29:07.791508Z"
    },
    "papermill": {
     "duration": 4583.110963,
     "end_time": "2023-10-30T23:29:07.794296",
     "exception": false,
     "start_time": "2023-10-30T22:12:44.683333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2130' max='2130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2130/2130 23:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Content Rmse</th>\n",
       "      <th>Wording Rmse</th>\n",
       "      <th>Mcrmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.444445</td>\n",
       "      <td>0.566231</td>\n",
       "      <td>0.753838</td>\n",
       "      <td>0.660034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.330485</td>\n",
       "      <td>0.510637</td>\n",
       "      <td>0.632629</td>\n",
       "      <td>0.571633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.447348</td>\n",
       "      <td>0.621960</td>\n",
       "      <td>0.534654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.278756</td>\n",
       "      <td>0.460050</td>\n",
       "      <td>0.588104</td>\n",
       "      <td>0.524077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248177</td>\n",
       "      <td>0.405870</td>\n",
       "      <td>0.575869</td>\n",
       "      <td>0.490869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.321220</td>\n",
       "      <td>0.419785</td>\n",
       "      <td>0.682803</td>\n",
       "      <td>0.551294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.243263</td>\n",
       "      <td>0.433609</td>\n",
       "      <td>0.546359</td>\n",
       "      <td>0.489984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.409734</td>\n",
       "      <td>0.622189</td>\n",
       "      <td>0.515961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.264230</td>\n",
       "      <td>0.473514</td>\n",
       "      <td>0.551584</td>\n",
       "      <td>0.512549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.283025</td>\n",
       "      <td>0.423460</td>\n",
       "      <td>0.621878</td>\n",
       "      <td>0.522669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.268830</td>\n",
       "      <td>0.466665</td>\n",
       "      <td>0.565582</td>\n",
       "      <td>0.516123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.276106</td>\n",
       "      <td>0.494623</td>\n",
       "      <td>0.554581</td>\n",
       "      <td>0.524602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.255901</td>\n",
       "      <td>0.455883</td>\n",
       "      <td>0.551336</td>\n",
       "      <td>0.503610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.277253</td>\n",
       "      <td>0.457200</td>\n",
       "      <td>0.587771</td>\n",
       "      <td>0.522485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.268713</td>\n",
       "      <td>0.490250</td>\n",
       "      <td>0.545052</td>\n",
       "      <td>0.517651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.253467</td>\n",
       "      <td>0.454012</td>\n",
       "      <td>0.548458</td>\n",
       "      <td>0.501235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.271086</td>\n",
       "      <td>0.505645</td>\n",
       "      <td>0.535253</td>\n",
       "      <td>0.520449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.271226</td>\n",
       "      <td>0.501993</td>\n",
       "      <td>0.538939</td>\n",
       "      <td>0.520466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.248335</td>\n",
       "      <td>0.452368</td>\n",
       "      <td>0.540401</td>\n",
       "      <td>0.496385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.265639</td>\n",
       "      <td>0.477775</td>\n",
       "      <td>0.550462</td>\n",
       "      <td>0.514118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.270386</td>\n",
       "      <td>0.470871</td>\n",
       "      <td>0.564847</td>\n",
       "      <td>0.517859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.254264</td>\n",
       "      <td>0.445469</td>\n",
       "      <td>0.556853</td>\n",
       "      <td>0.501161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.249915</td>\n",
       "      <td>0.425241</td>\n",
       "      <td>0.564800</td>\n",
       "      <td>0.495021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.253231</td>\n",
       "      <td>0.429031</td>\n",
       "      <td>0.567798</td>\n",
       "      <td>0.498415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.259335</td>\n",
       "      <td>0.468435</td>\n",
       "      <td>0.547026</td>\n",
       "      <td>0.507731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.263169</td>\n",
       "      <td>0.470385</td>\n",
       "      <td>0.552336</td>\n",
       "      <td>0.511361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.255184</td>\n",
       "      <td>0.460459</td>\n",
       "      <td>0.546211</td>\n",
       "      <td>0.503335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.257232</td>\n",
       "      <td>0.464097</td>\n",
       "      <td>0.546880</td>\n",
       "      <td>0.505489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2150' max='2150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2150/2150 26:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Content Rmse</th>\n",
       "      <th>Wording Rmse</th>\n",
       "      <th>Mcrmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.599277</td>\n",
       "      <td>0.687909</td>\n",
       "      <td>0.851667</td>\n",
       "      <td>0.769788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.976132</td>\n",
       "      <td>0.828361</td>\n",
       "      <td>1.125203</td>\n",
       "      <td>0.976782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.757838</td>\n",
       "      <td>0.707437</td>\n",
       "      <td>1.007576</td>\n",
       "      <td>0.857506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.552271</td>\n",
       "      <td>0.585354</td>\n",
       "      <td>0.872870</td>\n",
       "      <td>0.729112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.647894</td>\n",
       "      <td>0.622239</td>\n",
       "      <td>0.953208</td>\n",
       "      <td>0.787724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.531382</td>\n",
       "      <td>0.594194</td>\n",
       "      <td>0.842435</td>\n",
       "      <td>0.718315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.395600</td>\n",
       "      <td>0.544914</td>\n",
       "      <td>0.595120</td>\n",
       "      <td>0.857706</td>\n",
       "      <td>0.726413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.395600</td>\n",
       "      <td>0.506840</td>\n",
       "      <td>0.522586</td>\n",
       "      <td>0.860572</td>\n",
       "      <td>0.691579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.395600</td>\n",
       "      <td>0.475758</td>\n",
       "      <td>0.556611</td>\n",
       "      <td>0.801062</td>\n",
       "      <td>0.678836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.395600</td>\n",
       "      <td>0.526155</td>\n",
       "      <td>0.549682</td>\n",
       "      <td>0.866119</td>\n",
       "      <td>0.707900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.395600</td>\n",
       "      <td>0.500666</td>\n",
       "      <td>0.558989</td>\n",
       "      <td>0.829978</td>\n",
       "      <td>0.694483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.395600</td>\n",
       "      <td>0.519163</td>\n",
       "      <td>0.591527</td>\n",
       "      <td>0.829712</td>\n",
       "      <td>0.710620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.395600</td>\n",
       "      <td>0.495211</td>\n",
       "      <td>0.548179</td>\n",
       "      <td>0.830615</td>\n",
       "      <td>0.689397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.536865</td>\n",
       "      <td>0.557836</td>\n",
       "      <td>0.873240</td>\n",
       "      <td>0.715538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.519839</td>\n",
       "      <td>0.573100</td>\n",
       "      <td>0.843347</td>\n",
       "      <td>0.708224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.428882</td>\n",
       "      <td>0.538668</td>\n",
       "      <td>0.753394</td>\n",
       "      <td>0.646031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.501850</td>\n",
       "      <td>0.542543</td>\n",
       "      <td>0.842228</td>\n",
       "      <td>0.692385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.602122</td>\n",
       "      <td>0.571689</td>\n",
       "      <td>0.936705</td>\n",
       "      <td>0.754197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.538194</td>\n",
       "      <td>0.552107</td>\n",
       "      <td>0.878388</td>\n",
       "      <td>0.715248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.494772</td>\n",
       "      <td>0.534030</td>\n",
       "      <td>0.839259</td>\n",
       "      <td>0.686644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.459028</td>\n",
       "      <td>0.537420</td>\n",
       "      <td>0.793244</td>\n",
       "      <td>0.665332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.527632</td>\n",
       "      <td>0.539358</td>\n",
       "      <td>0.874275</td>\n",
       "      <td>0.706817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.475066</td>\n",
       "      <td>0.531405</td>\n",
       "      <td>0.817155</td>\n",
       "      <td>0.674280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.464976</td>\n",
       "      <td>0.532827</td>\n",
       "      <td>0.803770</td>\n",
       "      <td>0.668299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.441982</td>\n",
       "      <td>0.529194</td>\n",
       "      <td>0.777122</td>\n",
       "      <td>0.653158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.490980</td>\n",
       "      <td>0.526835</td>\n",
       "      <td>0.839288</td>\n",
       "      <td>0.683062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.193900</td>\n",
       "      <td>0.491546</td>\n",
       "      <td>0.517671</td>\n",
       "      <td>0.845641</td>\n",
       "      <td>0.681656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.193900</td>\n",
       "      <td>0.498365</td>\n",
       "      <td>0.533895</td>\n",
       "      <td>0.843615</td>\n",
       "      <td>0.688755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1695' max='1695' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1695/1695 24:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Content Rmse</th>\n",
       "      <th>Wording Rmse</th>\n",
       "      <th>Mcrmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.568230</td>\n",
       "      <td>0.617272</td>\n",
       "      <td>0.869158</td>\n",
       "      <td>0.743215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.540973</td>\n",
       "      <td>0.664080</td>\n",
       "      <td>0.800590</td>\n",
       "      <td>0.732335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.437287</td>\n",
       "      <td>0.569459</td>\n",
       "      <td>0.741816</td>\n",
       "      <td>0.655638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.424209</td>\n",
       "      <td>0.578906</td>\n",
       "      <td>0.716439</td>\n",
       "      <td>0.647673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.423824</td>\n",
       "      <td>0.559799</td>\n",
       "      <td>0.730939</td>\n",
       "      <td>0.645369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.460698</td>\n",
       "      <td>0.618401</td>\n",
       "      <td>0.734150</td>\n",
       "      <td>0.676275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.387733</td>\n",
       "      <td>0.543911</td>\n",
       "      <td>0.692550</td>\n",
       "      <td>0.618231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.415467</td>\n",
       "      <td>0.587665</td>\n",
       "      <td>0.696838</td>\n",
       "      <td>0.642251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.383383</td>\n",
       "      <td>0.523293</td>\n",
       "      <td>0.702089</td>\n",
       "      <td>0.612691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.409974</td>\n",
       "      <td>0.593241</td>\n",
       "      <td>0.684115</td>\n",
       "      <td>0.638678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.366439</td>\n",
       "      <td>0.532138</td>\n",
       "      <td>0.670603</td>\n",
       "      <td>0.601370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.360163</td>\n",
       "      <td>0.545462</td>\n",
       "      <td>0.650228</td>\n",
       "      <td>0.597845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.370417</td>\n",
       "      <td>0.560316</td>\n",
       "      <td>0.653361</td>\n",
       "      <td>0.606838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.361316</td>\n",
       "      <td>0.551456</td>\n",
       "      <td>0.646937</td>\n",
       "      <td>0.599197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.347440</td>\n",
       "      <td>0.535074</td>\n",
       "      <td>0.639199</td>\n",
       "      <td>0.587136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.377726</td>\n",
       "      <td>0.547133</td>\n",
       "      <td>0.675350</td>\n",
       "      <td>0.611242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.385975</td>\n",
       "      <td>0.582911</td>\n",
       "      <td>0.657391</td>\n",
       "      <td>0.620151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.378819</td>\n",
       "      <td>0.578869</td>\n",
       "      <td>0.650038</td>\n",
       "      <td>0.614453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.393421</td>\n",
       "      <td>0.614264</td>\n",
       "      <td>0.639939</td>\n",
       "      <td>0.627102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.378822</td>\n",
       "      <td>0.581180</td>\n",
       "      <td>0.647976</td>\n",
       "      <td>0.614578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.380657</td>\n",
       "      <td>0.588954</td>\n",
       "      <td>0.643776</td>\n",
       "      <td>0.616365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.360750</td>\n",
       "      <td>0.553033</td>\n",
       "      <td>0.644712</td>\n",
       "      <td>0.598873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test_pred'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_pred_debertav3base</th>\n",
       "      <th>wording_pred_debertav3base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.749956</td>\n",
       "      <td>-1.373214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.740249</td>\n",
       "      <td>-1.368435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.776103</td>\n",
       "      <td>-1.398595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.763828</td>\n",
       "      <td>-1.384381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content_pred_debertav3base  wording_pred_debertav3base\n",
       "0                   -1.749956                   -1.373214\n",
       "1                   -1.740249                   -1.368435\n",
       "2                   -1.776103                   -1.398595\n",
       "3                   -1.763828                   -1.384381"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv mcrmse: {'content_rmse': 0.5090863049854405, 'wording_rmse': 0.6492808878905182, 'mcrmse': 0.5791835964379793, 'preproc_type': 4}\n"
     ]
    }
   ],
   "source": [
    "oof_train, pred_dict, test_pred  = get_oof_pred_n_test(train,\n",
    "                                            model_name=CFG.model_name,\n",
    "                                            preproc_type=CFG.preproc_type,\n",
    "                                            learning_rate=CFG.learning_rate,\n",
    "                                            warmup_ratio=CFG.warmup_ratio,\n",
    "                                            hidden_dropout_prob=CFG.hidden_dropout_prob,\n",
    "                                            attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n",
    "                                            num_layers_to_freeze=CFG.num_layers_to_freeze,\n",
    "                                            weight_decay=CFG.weight_decay,\n",
    "                                            num_train_epochs=CFG.num_train_epochs,\n",
    "                                            n_splits=CFG.n_splits,\n",
    "                                            batch_size=CFG.batch_size,\n",
    "                                            random_seed=CFG.random_seed,\n",
    "                                            save_steps=CFG.save_steps,\n",
    "                                            max_length=CFG.max_length\n",
    "                                           )\n",
    "\n",
    "test=pd.concat([test, test_pred[['content_pred_debertav3base', 'wording_pred_debertav3base']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c87662",
   "metadata": {
    "papermill": {
     "duration": 0.058166,
     "end_time": "2023-10-30T23:29:07.911652",
     "exception": false,
     "start_time": "2023-10-30T23:29:07.853486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Merge results of bert-v3-base, bert-v3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02d9e3ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T23:29:08.031624Z",
     "iopub.status.busy": "2023-10-30T23:29:08.030860Z",
     "iopub.status.idle": "2023-10-30T23:29:08.042734Z",
     "shell.execute_reply": "2023-10-30T23:29:08.041943Z"
    },
    "papermill": {
     "duration": 0.073071,
     "end_time": "2023-10-30T23:29:08.044719",
     "exception": false,
     "start_time": "2023-10-30T23:29:07.971648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test=pd.concat([test, final_preds_test_bert_large], axis=1)\n",
    "\n",
    "oof_bert_large=oof_bert_large.rename(columns={\"pred_content\": \"content_pred_debertav3large\",\n",
    "                                              \"pred_wording\": \"wording_pred_debertav3large\"})\n",
    "\n",
    "train=pd.concat([train, oof_bert_large[['content_pred_debertav3large', \n",
    "                                        'wording_pred_debertav3large']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b7fba51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T23:29:08.161851Z",
     "iopub.status.busy": "2023-10-30T23:29:08.161368Z",
     "iopub.status.idle": "2023-10-30T23:29:08.171247Z",
     "shell.execute_reply": "2023-10-30T23:29:08.170461Z"
    },
    "papermill": {
     "duration": 0.070303,
     "end_time": "2023-10-30T23:29:08.173191",
     "exception": false,
     "start_time": "2023-10-30T23:29:08.102888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>-1.749956</td>\n",
       "      <td>-1.373214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>-1.740249</td>\n",
       "      <td>-1.368435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>-1.776103</td>\n",
       "      <td>-1.398595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>-1.763828</td>\n",
       "      <td>-1.384381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id   content   wording\n",
       "0  000000ffffff -1.749956 -1.373214\n",
       "1  111111eeeeee -1.740249 -1.368435\n",
       "2  222222cccccc -1.776103 -1.398595\n",
       "3  333333dddddd -1.763828 -1.384381"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[\"content\"] = test_pred.values[:, 0]\n",
    "sample_submission[\"wording\"] = test_pred.values[:, 1]\n",
    "\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a8b2495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T23:29:08.293520Z",
     "iopub.status.busy": "2023-10-30T23:29:08.293041Z",
     "iopub.status.idle": "2023-10-30T23:29:08.325768Z",
     "shell.execute_reply": "2023-10-30T23:29:08.324914Z"
    },
    "papermill": {
     "duration": 0.093626,
     "end_time": "2023-10-30T23:29:08.327585",
     "exception": false,
     "start_time": "2023-10-30T23:29:08.233959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>...</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg_prompt</th>\n",
       "      <th>neu_prompt</th>\n",
       "      <th>pos_prompt</th>\n",
       "      <th>compound_prompt</th>\n",
       "      <th>fold</th>\n",
       "      <th>content_pred_debertav3base</th>\n",
       "      <th>wording_pred_debertav3base</th>\n",
       "      <th>content_pred_debertav3large</th>\n",
       "      <th>wording_pred_debertav3large</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.103025</td>\n",
       "      <td>0.491970</td>\n",
       "      <td>0.836136</td>\n",
       "      <td>0.151125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0020ae56ffbf</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>-0.548304</td>\n",
       "      <td>0.506755</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4310</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.9949</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.734253</td>\n",
       "      <td>-0.290406</td>\n",
       "      <td>-1.300229</td>\n",
       "      <td>-0.716050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004e978e639e</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>3.128928</td>\n",
       "      <td>4.231226</td>\n",
       "      <td>269</td>\n",
       "      <td>32</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.9283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.155892</td>\n",
       "      <td>2.109924</td>\n",
       "      <td>0.192420</td>\n",
       "      <td>0.089422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005ab0199905</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.9283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.731279</td>\n",
       "      <td>-0.838590</td>\n",
       "      <td>-1.039375</td>\n",
       "      <td>-0.897786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0070c9e7af47</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>3.272894</td>\n",
       "      <td>3.219757</td>\n",
       "      <td>232</td>\n",
       "      <td>29</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.700438</td>\n",
       "      <td>1.802329</td>\n",
       "      <td>-0.006736</td>\n",
       "      <td>0.227909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>ff7c7e70df07</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They used all sorts of chemical concoctions to...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.9949</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.291691</td>\n",
       "      <td>-0.270563</td>\n",
       "      <td>-0.363451</td>\n",
       "      <td>-0.328194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>ffc34d056498</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The lowest classes are slaves and farmers slav...</td>\n",
       "      <td>-0.308448</td>\n",
       "      <td>0.048171</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6808</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.9283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.153692</td>\n",
       "      <td>-0.074165</td>\n",
       "      <td>-0.835059</td>\n",
       "      <td>-0.137106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>ffd1576d2e1b</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>they sorta made people start workin...</td>\n",
       "      <td>-1.408180</td>\n",
       "      <td>-0.493603</td>\n",
       "      <td>51</td>\n",
       "      <td>10</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.9283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.618375</td>\n",
       "      <td>-0.463430</td>\n",
       "      <td>0.335535</td>\n",
       "      <td>0.958109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>ffe4a98093b2</td>\n",
       "      <td>39c16e</td>\n",
       "      <td>An ideal tragety has three elements that make ...</td>\n",
       "      <td>-0.393310</td>\n",
       "      <td>0.627128</td>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.9969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030181</td>\n",
       "      <td>0.400283</td>\n",
       "      <td>-0.360416</td>\n",
       "      <td>-0.371727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>fffbccfd8a08</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>The meat would smell sour but the would \"rub i...</td>\n",
       "      <td>1.771596</td>\n",
       "      <td>0.547742</td>\n",
       "      <td>114</td>\n",
       "      <td>9</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7469</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.9949</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.367007</td>\n",
       "      <td>0.526235</td>\n",
       "      <td>-1.135331</td>\n",
       "      <td>-0.958692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7165 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        student_id prompt_id  \\\n",
       "0     000e8c3c7ddb    814d6b   \n",
       "1     0020ae56ffbf    ebad26   \n",
       "2     004e978e639e    3b9047   \n",
       "3     005ab0199905    3b9047   \n",
       "4     0070c9e7af47    814d6b   \n",
       "...            ...       ...   \n",
       "7160  ff7c7e70df07    ebad26   \n",
       "7161  ffc34d056498    3b9047   \n",
       "7162  ffd1576d2e1b    3b9047   \n",
       "7163  ffe4a98093b2    39c16e   \n",
       "7164  fffbccfd8a08    ebad26   \n",
       "\n",
       "                                                   text   content   wording  \\\n",
       "0     The third wave was an experimentto see how peo...  0.205683  0.380538   \n",
       "1     They would rub it up with soda to make the sme... -0.548304  0.506755   \n",
       "2     In Egypt, there were many occupations and soci...  3.128928  4.231226   \n",
       "3     The highest class was Pharaohs these people we... -0.210614 -0.471415   \n",
       "4     The Third Wave developed  rapidly because the ...  3.272894  3.219757   \n",
       "...                                                 ...       ...       ...   \n",
       "7160  They used all sorts of chemical concoctions to...  0.205683  0.380538   \n",
       "7161  The lowest classes are slaves and farmers slav... -0.308448  0.048171   \n",
       "7162             they sorta made people start workin... -1.408180 -0.493603   \n",
       "7163  An ideal tragety has three elements that make ... -0.393310  0.627128   \n",
       "7164  The meat would smell sour but the would \"rub i...  1.771596  0.547742   \n",
       "\n",
       "      summary_length  splling_err_num  \\\n",
       "0                 64                5   \n",
       "1                 54                2   \n",
       "2                269               32   \n",
       "3                 28                5   \n",
       "4                232               29   \n",
       "...              ...              ...   \n",
       "7160              76                9   \n",
       "7161              49                7   \n",
       "7162              51               10   \n",
       "7163              63                5   \n",
       "7164             114                9   \n",
       "\n",
       "                                        prompt_question  \\\n",
       "0     Summarize how the Third Wave developed over su...   \n",
       "1     Summarize the various ways the factory would u...   \n",
       "2     In complete sentences, summarize the structure...   \n",
       "3     In complete sentences, summarize the structure...   \n",
       "4     Summarize how the Third Wave developed over su...   \n",
       "...                                                 ...   \n",
       "7160  Summarize the various ways the factory would u...   \n",
       "7161  In complete sentences, summarize the structure...   \n",
       "7162  In complete sentences, summarize the structure...   \n",
       "7163  Summarize at least 3 elements of an ideal trag...   \n",
       "7164  Summarize the various ways the factory would u...   \n",
       "\n",
       "                   prompt_title  \\\n",
       "0                The Third Wave   \n",
       "1       Excerpt from The Jungle   \n",
       "2     Egyptian Social Structure   \n",
       "3     Egyptian Social Structure   \n",
       "4                The Third Wave   \n",
       "...                         ...   \n",
       "7160    Excerpt from The Jungle   \n",
       "7161  Egyptian Social Structure   \n",
       "7162  Egyptian Social Structure   \n",
       "7163                 On Tragedy   \n",
       "7164    Excerpt from The Jungle   \n",
       "\n",
       "                                            prompt_text  ... compound  \\\n",
       "0     Background \\r\\nThe Third Wave experiment took ...  ...   0.7845   \n",
       "1     With one member trimming beef in a cannery, an...  ...   0.4310   \n",
       "2     Egyptian society was structured like a pyramid...  ...   0.9725   \n",
       "3     Egyptian society was structured like a pyramid...  ...   0.0000   \n",
       "4     Background \\r\\nThe Third Wave experiment took ...  ...   0.9696   \n",
       "...                                                 ...  ...      ...   \n",
       "7160  With one member trimming beef in a cannery, an...  ...   0.4601   \n",
       "7161  Egyptian society was structured like a pyramid...  ...  -0.6808   \n",
       "7162  Egyptian society was structured like a pyramid...  ...   0.4404   \n",
       "7163  Chapter 13 \\r\\nAs the sequel to what has alrea...  ...   0.9715   \n",
       "7164  With one member trimming beef in a cannery, an...  ...   0.7469   \n",
       "\n",
       "     neg_prompt neu_prompt pos_prompt compound_prompt  fold  \\\n",
       "0         0.027      0.873      0.100          0.9915   0.0   \n",
       "1         0.086      0.879      0.035         -0.9949   2.0   \n",
       "2         0.063      0.845      0.092          0.9283   1.0   \n",
       "3         0.063      0.845      0.092          0.9283   1.0   \n",
       "4         0.027      0.873      0.100          0.9915   0.0   \n",
       "...         ...        ...        ...             ...   ...   \n",
       "7160      0.086      0.879      0.035         -0.9949   2.0   \n",
       "7161      0.063      0.845      0.092          0.9283   1.0   \n",
       "7162      0.063      0.845      0.092          0.9283   1.0   \n",
       "7163      0.229      0.628      0.143         -0.9969   0.0   \n",
       "7164      0.086      0.879      0.035         -0.9949   2.0   \n",
       "\n",
       "      content_pred_debertav3base  wording_pred_debertav3base  \\\n",
       "0                      -0.103025                    0.491970   \n",
       "1                      -0.734253                   -0.290406   \n",
       "2                       2.155892                    2.109924   \n",
       "3                      -0.731279                   -0.838590   \n",
       "4                       1.700438                    1.802329   \n",
       "...                          ...                         ...   \n",
       "7160                   -0.291691                   -0.270563   \n",
       "7161                   -0.153692                   -0.074165   \n",
       "7162                   -0.618375                   -0.463430   \n",
       "7163                    0.030181                    0.400283   \n",
       "7164                    0.367007                    0.526235   \n",
       "\n",
       "      content_pred_debertav3large  wording_pred_debertav3large  \n",
       "0                        0.836136                     0.151125  \n",
       "1                       -1.300229                    -0.716050  \n",
       "2                        0.192420                     0.089422  \n",
       "3                       -1.039375                    -0.897786  \n",
       "4                       -0.006736                     0.227909  \n",
       "...                           ...                          ...  \n",
       "7160                    -0.363451                    -0.328194  \n",
       "7161                    -0.835059                    -0.137106  \n",
       "7162                     0.335535                     0.958109  \n",
       "7163                    -0.360416                    -0.371727  \n",
       "7164                    -1.135331                    -0.958692  \n",
       "\n",
       "[7165 rows x 66 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5613f9ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T23:29:08.446804Z",
     "iopub.status.busy": "2023-10-30T23:29:08.446613Z",
     "iopub.status.idle": "2023-10-30T23:29:08.469067Z",
     "shell.execute_reply": "2023-10-30T23:29:08.468270Z"
    },
    "papermill": {
     "duration": 0.083928,
     "end_time": "2023-10-30T23:29:08.471013",
     "exception": false,
     "start_time": "2023-10-30T23:29:08.387085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>...</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg_prompt</th>\n",
       "      <th>neu_prompt</th>\n",
       "      <th>pos_prompt</th>\n",
       "      <th>compound_prompt</th>\n",
       "      <th>content_pred_debertav3base</th>\n",
       "      <th>wording_pred_debertav3base</th>\n",
       "      <th>content_pred_debertav3large</th>\n",
       "      <th>wording_pred_debertav3large</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.749956</td>\n",
       "      <td>-1.373214</td>\n",
       "      <td>-1.409746</td>\n",
       "      <td>-1.039957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.740249</td>\n",
       "      <td>-1.368435</td>\n",
       "      <td>-1.445152</td>\n",
       "      <td>-1.026454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.776103</td>\n",
       "      <td>-1.398595</td>\n",
       "      <td>-1.418963</td>\n",
       "      <td>-1.021244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.763828</td>\n",
       "      <td>-1.384381</td>\n",
       "      <td>-1.416630</td>\n",
       "      <td>-1.014911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id            text  summary_length  splling_err_num  \\\n",
       "0  000000ffffff    abc123  Example text 1               3                0   \n",
       "1  111111eeeeee    def789  Example text 2               3                0   \n",
       "2  222222cccccc    abc123  Example text 3               3                0   \n",
       "3  333333dddddd    def789  Example text 4               3                0   \n",
       "\n",
       "  prompt_question     prompt_title       prompt_text title author  ...  pos  \\\n",
       "0    Summarize...  Example Title 1  Heading\\nText...   NaN    NaN  ...  0.0   \n",
       "1    Summarize...  Example Title 2  Heading\\nText...   NaN    NaN  ...  0.0   \n",
       "2    Summarize...  Example Title 1  Heading\\nText...   NaN    NaN  ...  0.0   \n",
       "3    Summarize...  Example Title 2  Heading\\nText...   NaN    NaN  ...  0.0   \n",
       "\n",
       "  compound neg_prompt  neu_prompt  pos_prompt  compound_prompt  \\\n",
       "0      0.0        0.0         1.0         0.0              0.0   \n",
       "1      0.0        0.0         1.0         0.0              0.0   \n",
       "2      0.0        0.0         1.0         0.0              0.0   \n",
       "3      0.0        0.0         1.0         0.0              0.0   \n",
       "\n",
       "   content_pred_debertav3base  wording_pred_debertav3base  \\\n",
       "0                   -1.749956                   -1.373214   \n",
       "1                   -1.740249                   -1.368435   \n",
       "2                   -1.776103                   -1.398595   \n",
       "3                   -1.763828                   -1.384381   \n",
       "\n",
       "   content_pred_debertav3large  wording_pred_debertav3large  \n",
       "0                    -1.409746                    -1.039957  \n",
       "1                    -1.445152                    -1.026454  \n",
       "2                    -1.418963                    -1.021244  \n",
       "3                    -1.416630                    -1.014911  \n",
       "\n",
       "[4 rows x 63 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91305a4",
   "metadata": {
    "papermill": {
     "duration": 0.05867,
     "end_time": "2023-10-30T23:29:08.589271",
     "exception": false,
     "start_time": "2023-10-30T23:29:08.530601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267778a1",
   "metadata": {
    "papermill": {
     "duration": 0.0588,
     "end_time": "2023-10-30T23:29:08.706922",
     "exception": false,
     "start_time": "2023-10-30T23:29:08.648122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Using LightGBM with hyperparameters selected by Optuna for final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4500892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T23:29:08.826509Z",
     "iopub.status.busy": "2023-10-30T23:29:08.826014Z",
     "iopub.status.idle": "2023-10-30T23:29:08.837115Z",
     "shell.execute_reply": "2023-10-30T23:29:08.836371Z"
    },
    "papermill": {
     "duration": 0.072766,
     "end_time": "2023-10-30T23:29:08.838990",
     "exception": false,
     "start_time": "2023-10-30T23:29:08.766224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['student_id', 'prompt_id', 'text', 'summary_length', 'splling_err_num',\n",
       "       'prompt_question', 'prompt_title', 'prompt_text', 'title', 'author',\n",
       "       'description', 'grade', 'genre', 'lexile', 'lexile_scaled', 'is_prose',\n",
       "       'author_type', 'author_frequency', 'prompt_length',\n",
       "       'gunning_fog_prompt', 'flesch_kincaid_grade_level_prompt',\n",
       "       'flesch_reading_ease_prompt', 'flesch_reading_ease', 'word_count',\n",
       "       'sentence_length', 'vocabulary_richness', 'word_count2',\n",
       "       'num_unq_words', 'num_chars', 'avg_word_length', 'comma_count',\n",
       "       'semicolon_count', 'length_ratio', 'word_overlap_count',\n",
       "       'bigram_overlap_count', 'bigram_overlap_ratio', 'trigram_overlap_count',\n",
       "       'trigram_overlap_ratio', 'quotes_count', 'exclamation_count',\n",
       "       'question_count', 'pos_mean', 'punctuation_sum', 'keyword_density',\n",
       "       'jaccard_similarity', 'sentiment_polarity', 'sentiment_subjectivity',\n",
       "       'text_similarity', 'gunning_fog', 'flesch_kincaid_grade_level',\n",
       "       'count_difficult_words', 'neg', 'neu', 'pos', 'compound', 'neg_prompt',\n",
       "       'neu_prompt', 'pos_prompt', 'compound_prompt',\n",
       "       'content_pred_debertav3base', 'wording_pred_debertav3base',\n",
       "       'content_pred_debertav3large', 'wording_pred_debertav3large'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'train'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['student_id', 'prompt_id', 'text', 'content', 'wording',\n",
       "       'summary_length', 'splling_err_num', 'prompt_question', 'prompt_title',\n",
       "       'prompt_text', 'title', 'author', 'description', 'grade', 'genre',\n",
       "       'lexile', 'lexile_scaled', 'is_prose', 'author_type',\n",
       "       'author_frequency', 'prompt_length', 'gunning_fog_prompt',\n",
       "       'flesch_kincaid_grade_level_prompt', 'flesch_reading_ease_prompt',\n",
       "       'flesch_reading_ease', 'word_count', 'sentence_length',\n",
       "       'vocabulary_richness', 'word_count2', 'num_unq_words', 'num_chars',\n",
       "       'avg_word_length', 'comma_count', 'semicolon_count', 'length_ratio',\n",
       "       'word_overlap_count', 'bigram_overlap_count', 'bigram_overlap_ratio',\n",
       "       'trigram_overlap_count', 'trigram_overlap_ratio', 'quotes_count',\n",
       "       'exclamation_count', 'question_count', 'pos_mean', 'punctuation_sum',\n",
       "       'keyword_density', 'jaccard_similarity', 'sentiment_polarity',\n",
       "       'sentiment_subjectivity', 'text_similarity', 'gunning_fog',\n",
       "       'flesch_kincaid_grade_level', 'count_difficult_words', 'neg', 'neu',\n",
       "       'pos', 'compound', 'neg_prompt', 'neu_prompt', 'pos_prompt',\n",
       "       'compound_prompt', 'fold', 'content_pred_debertav3base',\n",
       "       'wording_pred_debertav3base', 'content_pred_debertav3large',\n",
       "       'wording_pred_debertav3large'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display('test', test.columns)\n",
    "display('train', train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1129979c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T23:29:08.959676Z",
     "iopub.status.busy": "2023-10-30T23:29:08.959422Z",
     "iopub.status.idle": "2023-10-31T00:09:51.630136Z",
     "shell.execute_reply": "2023-10-31T00:09:51.629177Z"
    },
    "papermill": {
     "duration": 2442.734053,
     "end_time": "2023-10-31T00:09:51.632540",
     "exception": false,
     "start_time": "2023-10-30T23:29:08.898487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:  0\n",
      "Best trial: score 0.43504583076689035, params {'max_depth': 3, 'learning_rate': 0.08734800638290831, 'lambda_l1': 0.006155479181097573, 'lambda_l2': 0.638473839951889, 'num_leaves': 4, 'subsample': 0.7387453623206642, 'min_child_weight': 2.8401930052105584, 'n_estimators': 283}\n",
      "FOLD:  1\n",
      "Best trial: score 0.525061474280949, params {'max_depth': 5, 'learning_rate': 0.08970065691189727, 'lambda_l1': 3.891872081553169e-06, 'lambda_l2': 3.2521087865077332e-06, 'num_leaves': 6, 'subsample': 0.5290405688839201, 'min_child_weight': 2.139657322929932, 'n_estimators': 268}\n",
      "FOLD:  2\n",
      "Best trial: score 0.4174340698510413, params {'max_depth': 3, 'learning_rate': 0.09742677299781034, 'lambda_l1': 0.10817504398055518, 'lambda_l2': 0.00021152310182735742, 'num_leaves': 6, 'subsample': 0.5180097856796314, 'min_child_weight': 2.5092191035356386, 'n_estimators': 95}\n",
      "FOLD:  0\n",
      "Best trial: score 0.575139381824517, params {'max_depth': 5, 'learning_rate': 0.07558359830843414, 'lambda_l1': 2.0793237114178764e-06, 'lambda_l2': 7.568248405821475e-08, 'num_leaves': 6, 'subsample': 0.8046821809148731, 'min_child_weight': 3.3614469909838163, 'n_estimators': 301}\n",
      "FOLD:  1\n",
      "Best trial: score 0.6878645451630125, params {'max_depth': 6, 'learning_rate': 0.07121965231480032, 'lambda_l1': 1.3759146752003548, 'lambda_l2': 0.0001129243947218444, 'num_leaves': 44, 'subsample': 0.6995246957189764, 'min_child_weight': 2.1565600987875535, 'n_estimators': 202}\n",
      "FOLD:  2\n",
      "Best trial: score 0.4919948249618057, params {'max_depth': 2, 'learning_rate': 0.07242510472621783, 'lambda_l1': 0.024641939455325706, 'lambda_l2': 9.543007896789747e-06, 'num_leaves': 3, 'subsample': 0.7095374792109896, 'min_child_weight': 3.2472744389679002, 'n_estimators': 341}\n"
     ]
    }
   ],
   "source": [
    "targets = [\"content\", \"wording\"]\n",
    "\n",
    "drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\",\n",
    "                \"prompt_question\", \"prompt_title\", \n",
    "                \"prompt_text\",\"title\", \"author\", \"description\", \"genre\"\n",
    "               ] + targets\n",
    "\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "def objective(trial, X_train_cv, y_train_cv, X_eval_cv, y_eval_cv):\n",
    "    dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
    "    dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 10) \n",
    "    params = {\n",
    "        'boosting_type': 'goss',\n",
    "        'random_state': 0, \n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1), \n",
    "        'max_depth': max_depth,\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 2.5), \n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 2.5), \n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 2**max_depth - 1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.5, 5), \n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 350),\n",
    "        'verbosity': -1  # Add this line to suppress warnings and info messages\n",
    "    }\n",
    "\n",
    "    evaluation_results = {}\n",
    "    model = lgb.train(params,\n",
    "                      num_boost_round=70000, \n",
    "                      valid_names=['train', 'valid'],\n",
    "                      train_set=dtrain,\n",
    "                      valid_sets=dval,\n",
    "                      verbose_eval=0,\n",
    "                      early_stopping_rounds=80,\n",
    "                      callbacks=[lgb.record_evaluation(evaluation_results)])\n",
    "\n",
    "    # Use the last metric for early stopping\n",
    "    evals_result = model.best_score\n",
    "    last_metric = list(evals_result.values())[-1]\n",
    "    # Save the model in the trial\n",
    "    trial.set_user_attr('best_model', model)  \n",
    "    return last_metric[list(last_metric.keys())[-1]]\n",
    "\n",
    "model_dict = {}\n",
    "\n",
    "for target in targets:\n",
    "    models = []\n",
    "    \n",
    "    for fold in range(CFG.n_splits):\n",
    "        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n",
    "        y_train_cv = train[train[\"fold\"] != fold][target]\n",
    "\n",
    "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective(trial, X_train_cv, y_train_cv, X_eval_cv, y_eval_cv), n_trials=1000) \n",
    "        print('FOLD: ', fold)\n",
    "        print('Best trial: score {}, params {}'.format(study.best_value, study.best_params))\n",
    "\n",
    "        best_model = study.trials[study.best_trial.number].user_attrs['best_model']\n",
    "        models.append(best_model)\n",
    "    \n",
    "    model_dict[target] = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de0d3184",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T00:09:51.754634Z",
     "iopub.status.busy": "2023-10-31T00:09:51.754408Z",
     "iopub.status.idle": "2023-10-31T00:09:51.760785Z",
     "shell.execute_reply": "2023-10-31T00:09:51.759520Z"
    },
    "papermill": {
     "duration": 0.069692,
     "end_time": "2023-10-31T00:09:51.763196",
     "exception": false,
     "start_time": "2023-10-31T00:09:51.693504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': [<lightgbm.basic.Booster at 0x7c4cac808dc0>,\n",
       "  <lightgbm.basic.Booster at 0x7c4cb94f8460>,\n",
       "  <lightgbm.basic.Booster at 0x7c4cac7e6ad0>],\n",
       " 'wording': [<lightgbm.basic.Booster at 0x7c4cb993f340>,\n",
       "  <lightgbm.basic.Booster at 0x7c4cba9d8dc0>,\n",
       "  <lightgbm.basic.Booster at 0x7c4cba7926e0>]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9947306a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T00:09:51.885639Z",
     "iopub.status.busy": "2023-10-31T00:09:51.885427Z",
     "iopub.status.idle": "2023-10-31T00:09:51.981935Z",
     "shell.execute_reply": "2023-10-31T00:09:51.980782Z"
    },
    "papermill": {
     "duration": 0.159709,
     "end_time": "2023-10-31T00:09:51.984033",
     "exception": false,
     "start_time": "2023-10-31T00:09:51.824324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_rmse : 0.45750967435324746\n",
      "wording_rmse : 0.5882075417833349\n",
      "mcrmse : 0.5228586080682912\n"
     ]
    }
   ],
   "source": [
    "rmses = []\n",
    "\n",
    "for target in targets:\n",
    "    models = model_dict[target]\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    \n",
    "    for fold, model in enumerate(models):\n",
    "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "        pred = model.predict(X_eval_cv)\n",
    "\n",
    "        trues.extend(y_eval_cv)\n",
    "        preds.extend(pred)\n",
    "        \n",
    "    rmse = np.sqrt(mean_squared_error(trues, preds))\n",
    "    print(f\"{target}_rmse : {rmse}\")\n",
    "    rmses = rmses + [rmse]\n",
    "\n",
    "print(f\"mcrmse : {sum(rmses) / len(rmses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b245795",
   "metadata": {
    "papermill": {
     "duration": 0.059873,
     "end_time": "2023-10-31T00:09:52.104845",
     "exception": false,
     "start_time": "2023-10-31T00:09:52.044972",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c88e54ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T00:09:52.227538Z",
     "iopub.status.busy": "2023-10-31T00:09:52.227271Z",
     "iopub.status.idle": "2023-10-31T00:09:52.239040Z",
     "shell.execute_reply": "2023-10-31T00:09:52.238280Z"
    },
    "papermill": {
     "duration": 0.075078,
     "end_time": "2023-10-31T00:09:52.240944",
     "exception": false,
     "start_time": "2023-10-31T00:09:52.165866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['student_id', 'prompt_id', 'text', 'summary_length', 'splling_err_num',\n",
       "       'prompt_question', 'prompt_title', 'prompt_text', 'title', 'author',\n",
       "       'description', 'grade', 'genre', 'lexile', 'lexile_scaled', 'is_prose',\n",
       "       'author_type', 'author_frequency', 'prompt_length',\n",
       "       'gunning_fog_prompt', 'flesch_kincaid_grade_level_prompt',\n",
       "       'flesch_reading_ease_prompt', 'flesch_reading_ease', 'word_count',\n",
       "       'sentence_length', 'vocabulary_richness', 'word_count2',\n",
       "       'num_unq_words', 'num_chars', 'avg_word_length', 'comma_count',\n",
       "       'semicolon_count', 'length_ratio', 'word_overlap_count',\n",
       "       'bigram_overlap_count', 'bigram_overlap_ratio', 'trigram_overlap_count',\n",
       "       'trigram_overlap_ratio', 'quotes_count', 'exclamation_count',\n",
       "       'question_count', 'pos_mean', 'punctuation_sum', 'keyword_density',\n",
       "       'jaccard_similarity', 'sentiment_polarity', 'sentiment_subjectivity',\n",
       "       'text_similarity', 'gunning_fog', 'flesch_kincaid_grade_level',\n",
       "       'count_difficult_words', 'neg', 'neu', 'pos', 'compound', 'neg_prompt',\n",
       "       'neu_prompt', 'pos_prompt', 'compound_prompt',\n",
       "       'content_pred_debertav3base', 'wording_pred_debertav3base',\n",
       "       'content_pred_debertav3large', 'wording_pred_debertav3large'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'train'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['student_id', 'prompt_id', 'text', 'content', 'wording',\n",
       "       'summary_length', 'splling_err_num', 'prompt_question', 'prompt_title',\n",
       "       'prompt_text', 'title', 'author', 'description', 'grade', 'genre',\n",
       "       'lexile', 'lexile_scaled', 'is_prose', 'author_type',\n",
       "       'author_frequency', 'prompt_length', 'gunning_fog_prompt',\n",
       "       'flesch_kincaid_grade_level_prompt', 'flesch_reading_ease_prompt',\n",
       "       'flesch_reading_ease', 'word_count', 'sentence_length',\n",
       "       'vocabulary_richness', 'word_count2', 'num_unq_words', 'num_chars',\n",
       "       'avg_word_length', 'comma_count', 'semicolon_count', 'length_ratio',\n",
       "       'word_overlap_count', 'bigram_overlap_count', 'bigram_overlap_ratio',\n",
       "       'trigram_overlap_count', 'trigram_overlap_ratio', 'quotes_count',\n",
       "       'exclamation_count', 'question_count', 'pos_mean', 'punctuation_sum',\n",
       "       'keyword_density', 'jaccard_similarity', 'sentiment_polarity',\n",
       "       'sentiment_subjectivity', 'text_similarity', 'gunning_fog',\n",
       "       'flesch_kincaid_grade_level', 'count_difficult_words', 'neg', 'neu',\n",
       "       'pos', 'compound', 'neg_prompt', 'neu_prompt', 'pos_prompt',\n",
       "       'compound_prompt', 'fold', 'content_pred_debertav3base',\n",
       "       'wording_pred_debertav3base', 'content_pred_debertav3large',\n",
       "       'wording_pred_debertav3large'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display('test', test.columns)\n",
    "display('train', train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4853191c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T00:09:52.364319Z",
     "iopub.status.busy": "2023-10-31T00:09:52.364116Z",
     "iopub.status.idle": "2023-10-31T00:09:52.368163Z",
     "shell.execute_reply": "2023-10-31T00:09:52.367429Z"
    },
    "papermill": {
     "duration": 0.067746,
     "end_time": "2023-10-31T00:09:52.370000",
     "exception": false,
     "start_time": "2023-10-31T00:09:52.302254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "                \"student_id\", \"prompt_id\", \"text\",\n",
    "                \"prompt_question\", \"prompt_title\", \n",
    "                \"prompt_text\",\"title\", \"author\", \"description\", \"genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a5d134e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T00:09:52.494120Z",
     "iopub.status.busy": "2023-10-31T00:09:52.493857Z",
     "iopub.status.idle": "2023-10-31T00:09:52.592728Z",
     "shell.execute_reply": "2023-10-31T00:09:52.591835Z"
    },
    "papermill": {
     "duration": 0.163882,
     "end_time": "2023-10-31T00:09:52.594971",
     "exception": false,
     "start_time": "2023-10-31T00:09:52.431089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_prompt</th>\n",
       "      <th>compound_prompt</th>\n",
       "      <th>content_pred_debertav3base</th>\n",
       "      <th>wording_pred_debertav3base</th>\n",
       "      <th>content_pred_debertav3large</th>\n",
       "      <th>wording_pred_debertav3large</th>\n",
       "      <th>content_pred_0</th>\n",
       "      <th>content_pred_1</th>\n",
       "      <th>content_pred_2</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.749956</td>\n",
       "      <td>-1.373214</td>\n",
       "      <td>-1.409746</td>\n",
       "      <td>-1.039957</td>\n",
       "      <td>-1.409879</td>\n",
       "      <td>-1.811203</td>\n",
       "      <td>-1.298021</td>\n",
       "      <td>-1.274952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.740249</td>\n",
       "      <td>-1.368435</td>\n",
       "      <td>-1.445152</td>\n",
       "      <td>-1.026454</td>\n",
       "      <td>-1.409879</td>\n",
       "      <td>-1.811203</td>\n",
       "      <td>-1.298021</td>\n",
       "      <td>-1.274952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.776103</td>\n",
       "      <td>-1.398595</td>\n",
       "      <td>-1.418963</td>\n",
       "      <td>-1.021244</td>\n",
       "      <td>-1.409879</td>\n",
       "      <td>-1.811203</td>\n",
       "      <td>-1.298021</td>\n",
       "      <td>-1.274952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.763828</td>\n",
       "      <td>-1.384381</td>\n",
       "      <td>-1.416630</td>\n",
       "      <td>-1.014911</td>\n",
       "      <td>-1.409879</td>\n",
       "      <td>-1.811203</td>\n",
       "      <td>-1.298021</td>\n",
       "      <td>-1.274952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id            text  summary_length  splling_err_num  \\\n",
       "0  000000ffffff    abc123  Example text 1               3                0   \n",
       "1  111111eeeeee    def789  Example text 2               3                0   \n",
       "2  222222cccccc    abc123  Example text 3               3                0   \n",
       "3  333333dddddd    def789  Example text 4               3                0   \n",
       "\n",
       "  prompt_question     prompt_title       prompt_text title author  ...  \\\n",
       "0    Summarize...  Example Title 1  Heading\\nText...   NaN    NaN  ...   \n",
       "1    Summarize...  Example Title 2  Heading\\nText...   NaN    NaN  ...   \n",
       "2    Summarize...  Example Title 1  Heading\\nText...   NaN    NaN  ...   \n",
       "3    Summarize...  Example Title 2  Heading\\nText...   NaN    NaN  ...   \n",
       "\n",
       "  pos_prompt compound_prompt content_pred_debertav3base  \\\n",
       "0        0.0             0.0                  -1.749956   \n",
       "1        0.0             0.0                  -1.740249   \n",
       "2        0.0             0.0                  -1.776103   \n",
       "3        0.0             0.0                  -1.763828   \n",
       "\n",
       "   wording_pred_debertav3base  content_pred_debertav3large  \\\n",
       "0                   -1.373214                    -1.409746   \n",
       "1                   -1.368435                    -1.445152   \n",
       "2                   -1.398595                    -1.418963   \n",
       "3                   -1.384381                    -1.416630   \n",
       "\n",
       "   wording_pred_debertav3large  content_pred_0  content_pred_1  \\\n",
       "0                    -1.039957       -1.409879       -1.811203   \n",
       "1                    -1.026454       -1.409879       -1.811203   \n",
       "2                    -1.021244       -1.409879       -1.811203   \n",
       "3                    -1.014911       -1.409879       -1.811203   \n",
       "\n",
       "   content_pred_2   content  \n",
       "0       -1.298021 -1.274952  \n",
       "1       -1.298021 -1.274952  \n",
       "2       -1.298021 -1.274952  \n",
       "3       -1.298021 -1.274952  \n",
       "\n",
       "[4 rows x 67 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['student_id', 'prompt_id', 'text', 'summary_length', 'splling_err_num',\n",
       "       'prompt_question', 'prompt_title', 'prompt_text', 'title', 'author',\n",
       "       'description', 'grade', 'genre', 'lexile', 'lexile_scaled', 'is_prose',\n",
       "       'author_type', 'author_frequency', 'prompt_length',\n",
       "       'gunning_fog_prompt', 'flesch_kincaid_grade_level_prompt',\n",
       "       'flesch_reading_ease_prompt', 'flesch_reading_ease', 'word_count',\n",
       "       'sentence_length', 'vocabulary_richness', 'word_count2',\n",
       "       'num_unq_words', 'num_chars', 'avg_word_length', 'comma_count',\n",
       "       'semicolon_count', 'length_ratio', 'word_overlap_count',\n",
       "       'bigram_overlap_count', 'bigram_overlap_ratio', 'trigram_overlap_count',\n",
       "       'trigram_overlap_ratio', 'quotes_count', 'exclamation_count',\n",
       "       'question_count', 'pos_mean', 'punctuation_sum', 'keyword_density',\n",
       "       'jaccard_similarity', 'sentiment_polarity', 'sentiment_subjectivity',\n",
       "       'text_similarity', 'gunning_fog', 'flesch_kincaid_grade_level',\n",
       "       'count_difficult_words', 'neg', 'neu', 'pos', 'compound', 'neg_prompt',\n",
       "       'neu_prompt', 'pos_prompt', 'compound_prompt',\n",
       "       'content_pred_debertav3base', 'wording_pred_debertav3base',\n",
       "       'content_pred_debertav3large', 'wording_pred_debertav3large',\n",
       "       'content_pred_0', 'content_pred_1', 'content_pred_2', 'content'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>...</th>\n",
       "      <th>content_pred_debertav3large</th>\n",
       "      <th>wording_pred_debertav3large</th>\n",
       "      <th>content_pred_0</th>\n",
       "      <th>content_pred_1</th>\n",
       "      <th>content_pred_2</th>\n",
       "      <th>content</th>\n",
       "      <th>wording_pred_0</th>\n",
       "      <th>wording_pred_1</th>\n",
       "      <th>wording_pred_2</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.409746</td>\n",
       "      <td>-1.039957</td>\n",
       "      <td>-1.409879</td>\n",
       "      <td>-1.811203</td>\n",
       "      <td>-1.298021</td>\n",
       "      <td>-1.274952</td>\n",
       "      <td>-1.441521</td>\n",
       "      <td>-0.695733</td>\n",
       "      <td>-1.469365</td>\n",
       "      <td>-1.222101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.445152</td>\n",
       "      <td>-1.026454</td>\n",
       "      <td>-1.409879</td>\n",
       "      <td>-1.811203</td>\n",
       "      <td>-1.298021</td>\n",
       "      <td>-1.274952</td>\n",
       "      <td>-1.441521</td>\n",
       "      <td>-0.695733</td>\n",
       "      <td>-1.469365</td>\n",
       "      <td>-1.222101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.418963</td>\n",
       "      <td>-1.021244</td>\n",
       "      <td>-1.409879</td>\n",
       "      <td>-1.811203</td>\n",
       "      <td>-1.298021</td>\n",
       "      <td>-1.274952</td>\n",
       "      <td>-1.441521</td>\n",
       "      <td>-0.695733</td>\n",
       "      <td>-1.469365</td>\n",
       "      <td>-1.222101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.416630</td>\n",
       "      <td>-1.014911</td>\n",
       "      <td>-1.409879</td>\n",
       "      <td>-1.811203</td>\n",
       "      <td>-1.298021</td>\n",
       "      <td>-1.274952</td>\n",
       "      <td>-1.441521</td>\n",
       "      <td>-0.695733</td>\n",
       "      <td>-1.469365</td>\n",
       "      <td>-1.222101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id            text  summary_length  splling_err_num  \\\n",
       "0  000000ffffff    abc123  Example text 1               3                0   \n",
       "1  111111eeeeee    def789  Example text 2               3                0   \n",
       "2  222222cccccc    abc123  Example text 3               3                0   \n",
       "3  333333dddddd    def789  Example text 4               3                0   \n",
       "\n",
       "  prompt_question     prompt_title       prompt_text title author  ...  \\\n",
       "0    Summarize...  Example Title 1  Heading\\nText...   NaN    NaN  ...   \n",
       "1    Summarize...  Example Title 2  Heading\\nText...   NaN    NaN  ...   \n",
       "2    Summarize...  Example Title 1  Heading\\nText...   NaN    NaN  ...   \n",
       "3    Summarize...  Example Title 2  Heading\\nText...   NaN    NaN  ...   \n",
       "\n",
       "  content_pred_debertav3large wording_pred_debertav3large content_pred_0  \\\n",
       "0                   -1.409746                   -1.039957      -1.409879   \n",
       "1                   -1.445152                   -1.026454      -1.409879   \n",
       "2                   -1.418963                   -1.021244      -1.409879   \n",
       "3                   -1.416630                   -1.014911      -1.409879   \n",
       "\n",
       "   content_pred_1  content_pred_2   content  wording_pred_0  wording_pred_1  \\\n",
       "0       -1.811203       -1.298021 -1.274952       -1.441521       -0.695733   \n",
       "1       -1.811203       -1.298021 -1.274952       -1.441521       -0.695733   \n",
       "2       -1.811203       -1.298021 -1.274952       -1.441521       -0.695733   \n",
       "3       -1.811203       -1.298021 -1.274952       -1.441521       -0.695733   \n",
       "\n",
       "   wording_pred_2   wording  \n",
       "0       -1.469365 -1.222101  \n",
       "1       -1.469365 -1.222101  \n",
       "2       -1.469365 -1.222101  \n",
       "3       -1.469365 -1.222101  \n",
       "\n",
       "[4 rows x 71 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['student_id', 'prompt_id', 'text', 'summary_length', 'splling_err_num',\n",
       "       'prompt_question', 'prompt_title', 'prompt_text', 'title', 'author',\n",
       "       'description', 'grade', 'genre', 'lexile', 'lexile_scaled', 'is_prose',\n",
       "       'author_type', 'author_frequency', 'prompt_length',\n",
       "       'gunning_fog_prompt', 'flesch_kincaid_grade_level_prompt',\n",
       "       'flesch_reading_ease_prompt', 'flesch_reading_ease', 'word_count',\n",
       "       'sentence_length', 'vocabulary_richness', 'word_count2',\n",
       "       'num_unq_words', 'num_chars', 'avg_word_length', 'comma_count',\n",
       "       'semicolon_count', 'length_ratio', 'word_overlap_count',\n",
       "       'bigram_overlap_count', 'bigram_overlap_ratio', 'trigram_overlap_count',\n",
       "       'trigram_overlap_ratio', 'quotes_count', 'exclamation_count',\n",
       "       'question_count', 'pos_mean', 'punctuation_sum', 'keyword_density',\n",
       "       'jaccard_similarity', 'sentiment_polarity', 'sentiment_subjectivity',\n",
       "       'text_similarity', 'gunning_fog', 'flesch_kincaid_grade_level',\n",
       "       'count_difficult_words', 'neg', 'neu', 'pos', 'compound', 'neg_prompt',\n",
       "       'neu_prompt', 'pos_prompt', 'compound_prompt',\n",
       "       'content_pred_debertav3base', 'wording_pred_debertav3base',\n",
       "       'content_pred_debertav3large', 'wording_pred_debertav3large',\n",
       "       'content_pred_0', 'content_pred_1', 'content_pred_2', 'content',\n",
       "       'wording_pred_0', 'wording_pred_1', 'wording_pred_2', 'wording'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_dict = {}\n",
    "for target in targets:\n",
    "    models = model_dict[target]\n",
    "    preds = []\n",
    "\n",
    "    for fold, model in enumerate(models):\n",
    "        X_eval_cv = test.drop(columns=drop_columns)\n",
    "    \n",
    "        pred = model.predict(X_eval_cv)\n",
    "        preds.append(pred)\n",
    "    \n",
    "    pred_dict[target] = preds\n",
    "    \n",
    "    \n",
    "for target in targets:\n",
    "    preds = pred_dict[target]\n",
    "    for i, pred in enumerate(preds):\n",
    "        test[f\"{target}_pred_{i}\"] = pred\n",
    "\n",
    "    medians = test[[f'{target}_pred_{fold}' for fold in range(CFG.n_splits)]].median(axis=1)\n",
    "\n",
    "    std_devs = test[[f'{target}_pred_{fold}' for fold in range(CFG.n_splits)]].std(axis=1)\n",
    "    \n",
    "    adjusted_medians = medians + (CFG.adjustment_factor * std_devs)\n",
    "\n",
    "    test[target] = adjusted_medians\n",
    "\n",
    "    display(test)\n",
    "    \n",
    "    display(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ec5d597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T00:09:52.731347Z",
     "iopub.status.busy": "2023-10-31T00:09:52.731137Z",
     "iopub.status.idle": "2023-10-31T00:09:52.737564Z",
     "shell.execute_reply": "2023-10-31T00:09:52.736793Z"
    },
    "papermill": {
     "duration": 0.073723,
     "end_time": "2023-10-31T00:09:52.739574",
     "exception": false,
     "start_time": "2023-10-31T00:09:52.665851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7671.993119,
   "end_time": "2023-10-31T00:09:55.810138",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-30T22:02:03.817019",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
